{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Neural Networks",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pratikwatwani/Applied-Data-Science/blob/master/Session%2011/Neural%20Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5UVFoAmkzhH",
        "colab_type": "text"
      },
      "source": [
        "# Neural networks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OT9XSvkFkzhJ",
        "colab_type": "text"
      },
      "source": [
        "The purpose of the below is to classify days over years 2017-2018 by their corresponding mobility patterns between 10 zones in Taipei (quantified by an aggregated temporal network of subway ridership flows across the city)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtm0HTHDkzhK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "outputId": "774b85bf-a6df-4385-ca98-49cb3140a6ee"
      },
      "source": [
        "#use Python 3.7\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Input, Dense\n",
        "from keras.utils import np_utils"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FW2LITZVkzhN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#read the data\n",
        "TNet=pd.read_csv('https://raw.githubusercontent.com/pratikwatwani/Applied-Data-Science/master/data/taipeiD_TNet2.csv',header=None);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ox85yef9kzhP",
        "colab_type": "code",
        "outputId": "158983b1-0769-4bb9-cf01-4d7a9481a5e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        }
      },
      "source": [
        "TNet.head() \n",
        "#each row represents a 10x10 adjacency matrix of the normalized Taipei subway mobility network between 10 zones flattened into a 100x1 row corresponding to a single day\n",
        "#days start at jan-1-2017"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>60</th>\n",
              "      <th>61</th>\n",
              "      <th>62</th>\n",
              "      <th>63</th>\n",
              "      <th>64</th>\n",
              "      <th>65</th>\n",
              "      <th>66</th>\n",
              "      <th>67</th>\n",
              "      <th>68</th>\n",
              "      <th>69</th>\n",
              "      <th>70</th>\n",
              "      <th>71</th>\n",
              "      <th>72</th>\n",
              "      <th>73</th>\n",
              "      <th>74</th>\n",
              "      <th>75</th>\n",
              "      <th>76</th>\n",
              "      <th>77</th>\n",
              "      <th>78</th>\n",
              "      <th>79</th>\n",
              "      <th>80</th>\n",
              "      <th>81</th>\n",
              "      <th>82</th>\n",
              "      <th>83</th>\n",
              "      <th>84</th>\n",
              "      <th>85</th>\n",
              "      <th>86</th>\n",
              "      <th>87</th>\n",
              "      <th>88</th>\n",
              "      <th>89</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.017943</td>\n",
              "      <td>0.005415</td>\n",
              "      <td>0.003590</td>\n",
              "      <td>0.008316</td>\n",
              "      <td>0.007859</td>\n",
              "      <td>0.012942</td>\n",
              "      <td>0.012196</td>\n",
              "      <td>0.019543</td>\n",
              "      <td>0.001196</td>\n",
              "      <td>0.003327</td>\n",
              "      <td>0.004588</td>\n",
              "      <td>0.016362</td>\n",
              "      <td>0.003059</td>\n",
              "      <td>0.006420</td>\n",
              "      <td>0.009864</td>\n",
              "      <td>0.005530</td>\n",
              "      <td>0.007357</td>\n",
              "      <td>0.017389</td>\n",
              "      <td>0.000923</td>\n",
              "      <td>0.001672</td>\n",
              "      <td>0.002640</td>\n",
              "      <td>0.002878</td>\n",
              "      <td>0.003133</td>\n",
              "      <td>0.001715</td>\n",
              "      <td>0.003610</td>\n",
              "      <td>0.001157</td>\n",
              "      <td>0.002406</td>\n",
              "      <td>0.004315</td>\n",
              "      <td>0.000550</td>\n",
              "      <td>0.001456</td>\n",
              "      <td>0.008631</td>\n",
              "      <td>0.007123</td>\n",
              "      <td>0.002301</td>\n",
              "      <td>0.010586</td>\n",
              "      <td>0.007468</td>\n",
              "      <td>0.010193</td>\n",
              "      <td>0.010568</td>\n",
              "      <td>0.020925</td>\n",
              "      <td>0.001346</td>\n",
              "      <td>0.002716</td>\n",
              "      <td>...</td>\n",
              "      <td>0.010876</td>\n",
              "      <td>0.007325</td>\n",
              "      <td>0.002859</td>\n",
              "      <td>0.009160</td>\n",
              "      <td>0.013417</td>\n",
              "      <td>0.009071</td>\n",
              "      <td>0.050107</td>\n",
              "      <td>0.043340</td>\n",
              "      <td>0.000679</td>\n",
              "      <td>0.003823</td>\n",
              "      <td>0.013696</td>\n",
              "      <td>0.014299</td>\n",
              "      <td>0.005237</td>\n",
              "      <td>0.015900</td>\n",
              "      <td>0.025870</td>\n",
              "      <td>0.021652</td>\n",
              "      <td>0.035190</td>\n",
              "      <td>0.049923</td>\n",
              "      <td>0.002971</td>\n",
              "      <td>0.009171</td>\n",
              "      <td>0.001081</td>\n",
              "      <td>0.001064</td>\n",
              "      <td>0.000710</td>\n",
              "      <td>0.001091</td>\n",
              "      <td>0.003131</td>\n",
              "      <td>0.008141</td>\n",
              "      <td>0.000839</td>\n",
              "      <td>0.004099</td>\n",
              "      <td>0.009125</td>\n",
              "      <td>0.005163</td>\n",
              "      <td>0.002529</td>\n",
              "      <td>0.001533</td>\n",
              "      <td>0.001860</td>\n",
              "      <td>0.002375</td>\n",
              "      <td>0.005408</td>\n",
              "      <td>0.008922</td>\n",
              "      <td>0.003945</td>\n",
              "      <td>0.011075</td>\n",
              "      <td>0.005073</td>\n",
              "      <td>0.012708</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.021283</td>\n",
              "      <td>0.005215</td>\n",
              "      <td>0.003530</td>\n",
              "      <td>0.009359</td>\n",
              "      <td>0.007803</td>\n",
              "      <td>0.014288</td>\n",
              "      <td>0.011185</td>\n",
              "      <td>0.019044</td>\n",
              "      <td>0.001382</td>\n",
              "      <td>0.003499</td>\n",
              "      <td>0.004859</td>\n",
              "      <td>0.016886</td>\n",
              "      <td>0.003053</td>\n",
              "      <td>0.007339</td>\n",
              "      <td>0.009820</td>\n",
              "      <td>0.005745</td>\n",
              "      <td>0.006608</td>\n",
              "      <td>0.016490</td>\n",
              "      <td>0.001023</td>\n",
              "      <td>0.001866</td>\n",
              "      <td>0.002897</td>\n",
              "      <td>0.002929</td>\n",
              "      <td>0.002973</td>\n",
              "      <td>0.001817</td>\n",
              "      <td>0.003471</td>\n",
              "      <td>0.001210</td>\n",
              "      <td>0.002197</td>\n",
              "      <td>0.004039</td>\n",
              "      <td>0.000664</td>\n",
              "      <td>0.001655</td>\n",
              "      <td>0.009672</td>\n",
              "      <td>0.007348</td>\n",
              "      <td>0.002248</td>\n",
              "      <td>0.012551</td>\n",
              "      <td>0.007475</td>\n",
              "      <td>0.011087</td>\n",
              "      <td>0.009090</td>\n",
              "      <td>0.020644</td>\n",
              "      <td>0.001560</td>\n",
              "      <td>0.003032</td>\n",
              "      <td>...</td>\n",
              "      <td>0.010139</td>\n",
              "      <td>0.006609</td>\n",
              "      <td>0.002760</td>\n",
              "      <td>0.008469</td>\n",
              "      <td>0.010956</td>\n",
              "      <td>0.009114</td>\n",
              "      <td>0.046897</td>\n",
              "      <td>0.038464</td>\n",
              "      <td>0.000647</td>\n",
              "      <td>0.003762</td>\n",
              "      <td>0.014380</td>\n",
              "      <td>0.016011</td>\n",
              "      <td>0.003765</td>\n",
              "      <td>0.017911</td>\n",
              "      <td>0.022929</td>\n",
              "      <td>0.021901</td>\n",
              "      <td>0.034270</td>\n",
              "      <td>0.040281</td>\n",
              "      <td>0.003776</td>\n",
              "      <td>0.009128</td>\n",
              "      <td>0.001298</td>\n",
              "      <td>0.001179</td>\n",
              "      <td>0.000663</td>\n",
              "      <td>0.001337</td>\n",
              "      <td>0.003490</td>\n",
              "      <td>0.008978</td>\n",
              "      <td>0.000753</td>\n",
              "      <td>0.004377</td>\n",
              "      <td>0.010360</td>\n",
              "      <td>0.005964</td>\n",
              "      <td>0.002803</td>\n",
              "      <td>0.001757</td>\n",
              "      <td>0.001783</td>\n",
              "      <td>0.002549</td>\n",
              "      <td>0.005515</td>\n",
              "      <td>0.009650</td>\n",
              "      <td>0.003596</td>\n",
              "      <td>0.009618</td>\n",
              "      <td>0.005946</td>\n",
              "      <td>0.013709</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.028988</td>\n",
              "      <td>0.006511</td>\n",
              "      <td>0.005591</td>\n",
              "      <td>0.012970</td>\n",
              "      <td>0.007816</td>\n",
              "      <td>0.015878</td>\n",
              "      <td>0.010973</td>\n",
              "      <td>0.015768</td>\n",
              "      <td>0.002252</td>\n",
              "      <td>0.005388</td>\n",
              "      <td>0.006879</td>\n",
              "      <td>0.013790</td>\n",
              "      <td>0.003706</td>\n",
              "      <td>0.009401</td>\n",
              "      <td>0.008878</td>\n",
              "      <td>0.006245</td>\n",
              "      <td>0.006937</td>\n",
              "      <td>0.013613</td>\n",
              "      <td>0.001545</td>\n",
              "      <td>0.002790</td>\n",
              "      <td>0.005575</td>\n",
              "      <td>0.003375</td>\n",
              "      <td>0.004373</td>\n",
              "      <td>0.003305</td>\n",
              "      <td>0.005246</td>\n",
              "      <td>0.001148</td>\n",
              "      <td>0.002829</td>\n",
              "      <td>0.003980</td>\n",
              "      <td>0.000803</td>\n",
              "      <td>0.002643</td>\n",
              "      <td>0.013623</td>\n",
              "      <td>0.008729</td>\n",
              "      <td>0.003722</td>\n",
              "      <td>0.015259</td>\n",
              "      <td>0.006972</td>\n",
              "      <td>0.012822</td>\n",
              "      <td>0.009648</td>\n",
              "      <td>0.017857</td>\n",
              "      <td>0.002672</td>\n",
              "      <td>0.004533</td>\n",
              "      <td>...</td>\n",
              "      <td>0.011613</td>\n",
              "      <td>0.007232</td>\n",
              "      <td>0.003313</td>\n",
              "      <td>0.009770</td>\n",
              "      <td>0.008924</td>\n",
              "      <td>0.009524</td>\n",
              "      <td>0.039863</td>\n",
              "      <td>0.029368</td>\n",
              "      <td>0.000581</td>\n",
              "      <td>0.005011</td>\n",
              "      <td>0.013882</td>\n",
              "      <td>0.013373</td>\n",
              "      <td>0.003620</td>\n",
              "      <td>0.017100</td>\n",
              "      <td>0.018839</td>\n",
              "      <td>0.018666</td>\n",
              "      <td>0.026413</td>\n",
              "      <td>0.030177</td>\n",
              "      <td>0.003673</td>\n",
              "      <td>0.008805</td>\n",
              "      <td>0.002030</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.000786</td>\n",
              "      <td>0.002192</td>\n",
              "      <td>0.004388</td>\n",
              "      <td>0.010398</td>\n",
              "      <td>0.000546</td>\n",
              "      <td>0.004129</td>\n",
              "      <td>0.011692</td>\n",
              "      <td>0.009807</td>\n",
              "      <td>0.004649</td>\n",
              "      <td>0.002555</td>\n",
              "      <td>0.002672</td>\n",
              "      <td>0.004291</td>\n",
              "      <td>0.007385</td>\n",
              "      <td>0.009558</td>\n",
              "      <td>0.004293</td>\n",
              "      <td>0.008791</td>\n",
              "      <td>0.010040</td>\n",
              "      <td>0.016301</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.029534</td>\n",
              "      <td>0.006471</td>\n",
              "      <td>0.005615</td>\n",
              "      <td>0.013017</td>\n",
              "      <td>0.007717</td>\n",
              "      <td>0.016098</td>\n",
              "      <td>0.011182</td>\n",
              "      <td>0.015815</td>\n",
              "      <td>0.002325</td>\n",
              "      <td>0.005443</td>\n",
              "      <td>0.006955</td>\n",
              "      <td>0.014044</td>\n",
              "      <td>0.003699</td>\n",
              "      <td>0.009330</td>\n",
              "      <td>0.008967</td>\n",
              "      <td>0.006290</td>\n",
              "      <td>0.007313</td>\n",
              "      <td>0.013566</td>\n",
              "      <td>0.001511</td>\n",
              "      <td>0.002833</td>\n",
              "      <td>0.005504</td>\n",
              "      <td>0.003456</td>\n",
              "      <td>0.004210</td>\n",
              "      <td>0.003239</td>\n",
              "      <td>0.005267</td>\n",
              "      <td>0.001098</td>\n",
              "      <td>0.002910</td>\n",
              "      <td>0.003914</td>\n",
              "      <td>0.000742</td>\n",
              "      <td>0.002519</td>\n",
              "      <td>0.013751</td>\n",
              "      <td>0.008552</td>\n",
              "      <td>0.003736</td>\n",
              "      <td>0.014924</td>\n",
              "      <td>0.006757</td>\n",
              "      <td>0.012755</td>\n",
              "      <td>0.009960</td>\n",
              "      <td>0.017484</td>\n",
              "      <td>0.002665</td>\n",
              "      <td>0.004498</td>\n",
              "      <td>...</td>\n",
              "      <td>0.011870</td>\n",
              "      <td>0.007473</td>\n",
              "      <td>0.003513</td>\n",
              "      <td>0.010152</td>\n",
              "      <td>0.009209</td>\n",
              "      <td>0.009930</td>\n",
              "      <td>0.041379</td>\n",
              "      <td>0.029797</td>\n",
              "      <td>0.000618</td>\n",
              "      <td>0.005187</td>\n",
              "      <td>0.013526</td>\n",
              "      <td>0.012225</td>\n",
              "      <td>0.003561</td>\n",
              "      <td>0.016417</td>\n",
              "      <td>0.018527</td>\n",
              "      <td>0.017725</td>\n",
              "      <td>0.025343</td>\n",
              "      <td>0.030699</td>\n",
              "      <td>0.003375</td>\n",
              "      <td>0.007993</td>\n",
              "      <td>0.002014</td>\n",
              "      <td>0.001469</td>\n",
              "      <td>0.000773</td>\n",
              "      <td>0.002228</td>\n",
              "      <td>0.004599</td>\n",
              "      <td>0.010936</td>\n",
              "      <td>0.000596</td>\n",
              "      <td>0.004077</td>\n",
              "      <td>0.012252</td>\n",
              "      <td>0.009988</td>\n",
              "      <td>0.004611</td>\n",
              "      <td>0.002473</td>\n",
              "      <td>0.002636</td>\n",
              "      <td>0.004195</td>\n",
              "      <td>0.007255</td>\n",
              "      <td>0.009487</td>\n",
              "      <td>0.004316</td>\n",
              "      <td>0.008729</td>\n",
              "      <td>0.010296</td>\n",
              "      <td>0.016437</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.029333</td>\n",
              "      <td>0.006525</td>\n",
              "      <td>0.005727</td>\n",
              "      <td>0.013098</td>\n",
              "      <td>0.007692</td>\n",
              "      <td>0.016358</td>\n",
              "      <td>0.011000</td>\n",
              "      <td>0.015677</td>\n",
              "      <td>0.002344</td>\n",
              "      <td>0.005527</td>\n",
              "      <td>0.006860</td>\n",
              "      <td>0.013586</td>\n",
              "      <td>0.003710</td>\n",
              "      <td>0.009243</td>\n",
              "      <td>0.008994</td>\n",
              "      <td>0.006580</td>\n",
              "      <td>0.007113</td>\n",
              "      <td>0.014127</td>\n",
              "      <td>0.001536</td>\n",
              "      <td>0.002922</td>\n",
              "      <td>0.005472</td>\n",
              "      <td>0.003366</td>\n",
              "      <td>0.004166</td>\n",
              "      <td>0.003233</td>\n",
              "      <td>0.005255</td>\n",
              "      <td>0.001152</td>\n",
              "      <td>0.002889</td>\n",
              "      <td>0.003851</td>\n",
              "      <td>0.000735</td>\n",
              "      <td>0.002525</td>\n",
              "      <td>0.013695</td>\n",
              "      <td>0.008369</td>\n",
              "      <td>0.003819</td>\n",
              "      <td>0.015103</td>\n",
              "      <td>0.006924</td>\n",
              "      <td>0.012921</td>\n",
              "      <td>0.009824</td>\n",
              "      <td>0.017778</td>\n",
              "      <td>0.002605</td>\n",
              "      <td>0.004648</td>\n",
              "      <td>...</td>\n",
              "      <td>0.011968</td>\n",
              "      <td>0.007428</td>\n",
              "      <td>0.003594</td>\n",
              "      <td>0.010037</td>\n",
              "      <td>0.009058</td>\n",
              "      <td>0.009952</td>\n",
              "      <td>0.040614</td>\n",
              "      <td>0.030371</td>\n",
              "      <td>0.000633</td>\n",
              "      <td>0.005188</td>\n",
              "      <td>0.013355</td>\n",
              "      <td>0.011994</td>\n",
              "      <td>0.003674</td>\n",
              "      <td>0.016204</td>\n",
              "      <td>0.018132</td>\n",
              "      <td>0.017880</td>\n",
              "      <td>0.025106</td>\n",
              "      <td>0.030883</td>\n",
              "      <td>0.003316</td>\n",
              "      <td>0.008219</td>\n",
              "      <td>0.002130</td>\n",
              "      <td>0.001476</td>\n",
              "      <td>0.000823</td>\n",
              "      <td>0.002186</td>\n",
              "      <td>0.004413</td>\n",
              "      <td>0.010712</td>\n",
              "      <td>0.000562</td>\n",
              "      <td>0.004160</td>\n",
              "      <td>0.011789</td>\n",
              "      <td>0.009981</td>\n",
              "      <td>0.004694</td>\n",
              "      <td>0.002515</td>\n",
              "      <td>0.002677</td>\n",
              "      <td>0.004222</td>\n",
              "      <td>0.007269</td>\n",
              "      <td>0.009921</td>\n",
              "      <td>0.004387</td>\n",
              "      <td>0.008923</td>\n",
              "      <td>0.010381</td>\n",
              "      <td>0.016914</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 100 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         0         1         2   ...        97        98        99\n",
              "0  0.017943  0.005415  0.003590  ...  0.011075  0.005073  0.012708\n",
              "1  0.021283  0.005215  0.003530  ...  0.009618  0.005946  0.013709\n",
              "2  0.028988  0.006511  0.005591  ...  0.008791  0.010040  0.016301\n",
              "3  0.029534  0.006471  0.005615  ...  0.008729  0.010296  0.016437\n",
              "4  0.029333  0.006525  0.005727  ...  0.008923  0.010381  0.016914\n",
              "\n",
              "[5 rows x 100 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYIKxV9BkzhS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#convert to an array and scale the data\n",
        "X=np.array(TNet);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHuKaNkHkzhU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X=MinMaxScaler(feature_range=(0, 1), copy=True).fit_transform(X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gu4LSECmkzhW",
        "colab_type": "code",
        "outputId": "c7c0dc71-56d6-49c4-8790-19e9cc6b7a74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(669, 100)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ErlgDYPBkzhY",
        "colab_type": "code",
        "outputId": "93c851a4-9106-479b-b6d5-f46e51691eec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#define day of the week corresponding to each day of observation; 0-Sunday, 1-Monday,...,6-Saturday\n",
        "y=np.array(range(669))%7; y[:10]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 2, 3, 4, 5, 6, 0, 1, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayvjP2wjkzha",
        "colab_type": "code",
        "outputId": "0b56d931-8b8b-40ba-b621-c58043fb06d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "yc=np_utils.to_categorical(y) #get categorical binary variables isSunday, isMonday,...\n",
        "yc[:5]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 1., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 1., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSB4kHhYkzhc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_test=X[400:,:]; X_train=X[:400,:]; #split the data into training and test\n",
        "y_test=yc[400:,:]; y_train=yc[:400,:]\n",
        "dim = X_train.shape[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8qkzBH1qqoW",
        "colab_type": "code",
        "outputId": "c29e5040-8531-44c6-c912-89e75463b889",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "print(X_test.shape)\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(269, 100)\n",
            "(400, 100)\n",
            "(400, 7)\n",
            "(269, 7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoVuc7aOkzhi",
        "colab_type": "text"
      },
      "source": [
        "## Task 1. Classify weekdays/weekends\n",
        "Label the rows with ones for weekends, zeros for weekdays.\n",
        "Train a neural network with 4 layers of 30,10,3 and 1 (output) neurons over the training sample against this label, evaluating its performance over the test sample. Report the acheived accuracy (categorical) over the test sample\n",
        "\n",
        "First three layers use relu activation function, last one - sigmoid.\n",
        "Use loss='binary_crossentropy', optimizer='adam', 100 epochs, batch_size=20. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_rdDQtRnbDy",
        "colab_type": "code",
        "outputId": "bdc30a7e-a981-4189-8fd3-ac5cf2df91f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        }
      },
      "source": [
        "y"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0,\n",
              "       1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1,\n",
              "       2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2,\n",
              "       3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3,\n",
              "       4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4,\n",
              "       5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5,\n",
              "       6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6,\n",
              "       0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0,\n",
              "       1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1,\n",
              "       2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2,\n",
              "       3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3,\n",
              "       4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4,\n",
              "       5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5,\n",
              "       6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6,\n",
              "       0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0,\n",
              "       1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1,\n",
              "       2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2,\n",
              "       3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3,\n",
              "       4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4,\n",
              "       5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5,\n",
              "       6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6,\n",
              "       0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0,\n",
              "       1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1,\n",
              "       2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2,\n",
              "       3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3,\n",
              "       4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4,\n",
              "       5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5,\n",
              "       6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6,\n",
              "       0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0,\n",
              "       1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1,\n",
              "       2, 3, 4, 5, 6, 0, 1, 2, 3])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIzdBDqsk-9k",
        "colab_type": "code",
        "outputId": "52a6ec89-be24-4811-df8e-d5d2d647326a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        }
      },
      "source": [
        "y_c = y\n",
        "for idx, item in enumerate(y_c):\n",
        "  if ((item == 0)| (item == 6)):\n",
        "    y_c[idx] = 1\n",
        "  else:\n",
        "    y_c[idx] = 0\n",
        "\n",
        "y_c"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,\n",
              "       0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,\n",
              "       0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
              "       0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
              "       0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
              "       0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
              "       1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n",
              "       1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,\n",
              "       0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,\n",
              "       0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
              "       0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
              "       0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
              "       0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
              "       1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n",
              "       1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,\n",
              "       0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,\n",
              "       0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
              "       0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
              "       0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
              "       0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
              "       1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n",
              "       1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,\n",
              "       0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,\n",
              "       0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
              "       0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
              "       0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
              "       0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
              "       1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n",
              "       1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,\n",
              "       0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,\n",
              "       0, 0, 0, 0, 1, 1, 0, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0W7JKnzHrZzF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y1_train=y_c[:400]\n",
        "y1_test=y_c[400:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4F6T__gq0C9",
        "colab_type": "code",
        "outputId": "80861723-d00e-4216-d5aa-ec505e055ba2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(y1_train.shape)\n",
        "print(y1_test.shape)\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(400,)\n",
            "(269,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNPlpyQQnwTy",
        "colab_type": "code",
        "outputId": "8145f4e0-69a9-43c4-9fb7-6948d64bc302",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "np.random.seed(1612)\n",
        "test = pd.DataFrame()\n",
        "model = Sequential()\n",
        "model.add(Dense(30, activation='relu', input_dim=dim))\n",
        "model.add(Dense(10, activation='relu'))\n",
        "model.add(Dense(3, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(X_train, y1_train, validation_data=(X_test, y1_test), epochs=100, batch_size=20, verbose=2)\n",
        "test = model.predict(X_test)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 400 samples, validate on 269 samples\n",
            "Epoch 1/100\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            " - 1s - loss: 0.4332 - acc: 0.7125 - val_loss: 0.3425 - val_acc: 0.7175\n",
            "Epoch 2/100\n",
            " - 0s - loss: 0.3222 - acc: 0.7125 - val_loss: 0.2891 - val_acc: 0.7175\n",
            "Epoch 3/100\n",
            " - 0s - loss: 0.2811 - acc: 0.7475 - val_loss: 0.2557 - val_acc: 0.8439\n",
            "Epoch 4/100\n",
            " - 0s - loss: 0.2363 - acc: 0.9500 - val_loss: 0.2099 - val_acc: 0.9554\n",
            "Epoch 5/100\n",
            " - 0s - loss: 0.2020 - acc: 0.9525 - val_loss: 0.1810 - val_acc: 0.9554\n",
            "Epoch 6/100\n",
            " - 0s - loss: 0.1744 - acc: 0.9525 - val_loss: 0.1585 - val_acc: 0.9554\n",
            "Epoch 7/100\n",
            " - 0s - loss: 0.1535 - acc: 0.9525 - val_loss: 0.1447 - val_acc: 0.9554\n",
            "Epoch 8/100\n",
            " - 0s - loss: 0.1417 - acc: 0.9525 - val_loss: 0.1367 - val_acc: 0.9554\n",
            "Epoch 9/100\n",
            " - 0s - loss: 0.1372 - acc: 0.9525 - val_loss: 0.1323 - val_acc: 0.9554\n",
            "Epoch 10/100\n",
            " - 0s - loss: 0.1333 - acc: 0.9525 - val_loss: 0.1299 - val_acc: 0.9554\n",
            "Epoch 11/100\n",
            " - 0s - loss: 0.1365 - acc: 0.9550 - val_loss: 0.1289 - val_acc: 0.9517\n",
            "Epoch 12/100\n",
            " - 0s - loss: 0.1292 - acc: 0.9525 - val_loss: 0.1264 - val_acc: 0.9554\n",
            "Epoch 13/100\n",
            " - 0s - loss: 0.1256 - acc: 0.9525 - val_loss: 0.1249 - val_acc: 0.9554\n",
            "Epoch 14/100\n",
            " - 0s - loss: 0.1222 - acc: 0.9525 - val_loss: 0.1229 - val_acc: 0.9554\n",
            "Epoch 15/100\n",
            " - 0s - loss: 0.1212 - acc: 0.9525 - val_loss: 0.1222 - val_acc: 0.9554\n",
            "Epoch 16/100\n",
            " - 0s - loss: 0.1233 - acc: 0.9550 - val_loss: 0.1195 - val_acc: 0.9554\n",
            "Epoch 17/100\n",
            " - 0s - loss: 0.1181 - acc: 0.9550 - val_loss: 0.1190 - val_acc: 0.9554\n",
            "Epoch 18/100\n",
            " - 0s - loss: 0.1115 - acc: 0.9525 - val_loss: 0.1160 - val_acc: 0.9554\n",
            "Epoch 19/100\n",
            " - 0s - loss: 0.1119 - acc: 0.9550 - val_loss: 0.1147 - val_acc: 0.9554\n",
            "Epoch 20/100\n",
            " - 0s - loss: 0.1105 - acc: 0.9550 - val_loss: 0.1127 - val_acc: 0.9554\n",
            "Epoch 21/100\n",
            " - 0s - loss: 0.1095 - acc: 0.9550 - val_loss: 0.1114 - val_acc: 0.9554\n",
            "Epoch 22/100\n",
            " - 0s - loss: 0.1043 - acc: 0.9550 - val_loss: 0.1100 - val_acc: 0.9554\n",
            "Epoch 23/100\n",
            " - 0s - loss: 0.1028 - acc: 0.9550 - val_loss: 0.1091 - val_acc: 0.9554\n",
            "Epoch 24/100\n",
            " - 0s - loss: 0.0992 - acc: 0.9550 - val_loss: 0.1056 - val_acc: 0.9591\n",
            "Epoch 25/100\n",
            " - 0s - loss: 0.0991 - acc: 0.9550 - val_loss: 0.1037 - val_acc: 0.9591\n",
            "Epoch 26/100\n",
            " - 0s - loss: 0.0928 - acc: 0.9575 - val_loss: 0.1015 - val_acc: 0.9628\n",
            "Epoch 27/100\n",
            " - 0s - loss: 0.0900 - acc: 0.9575 - val_loss: 0.1005 - val_acc: 0.9628\n",
            "Epoch 28/100\n",
            " - 0s - loss: 0.0885 - acc: 0.9575 - val_loss: 0.0994 - val_acc: 0.9628\n",
            "Epoch 29/100\n",
            " - 0s - loss: 0.0863 - acc: 0.9600 - val_loss: 0.0971 - val_acc: 0.9628\n",
            "Epoch 30/100\n",
            " - 0s - loss: 0.0826 - acc: 0.9575 - val_loss: 0.0959 - val_acc: 0.9628\n",
            "Epoch 31/100\n",
            " - 0s - loss: 0.0789 - acc: 0.9650 - val_loss: 0.0937 - val_acc: 0.9628\n",
            "Epoch 32/100\n",
            " - 0s - loss: 0.0772 - acc: 0.9625 - val_loss: 0.0930 - val_acc: 0.9628\n",
            "Epoch 33/100\n",
            " - 0s - loss: 0.0758 - acc: 0.9600 - val_loss: 0.0914 - val_acc: 0.9591\n",
            "Epoch 34/100\n",
            " - 0s - loss: 0.0721 - acc: 0.9625 - val_loss: 0.0908 - val_acc: 0.9591\n",
            "Epoch 35/100\n",
            " - 0s - loss: 0.0692 - acc: 0.9650 - val_loss: 0.0876 - val_acc: 0.9591\n",
            "Epoch 36/100\n",
            " - 0s - loss: 0.0654 - acc: 0.9700 - val_loss: 0.0920 - val_acc: 0.9665\n",
            "Epoch 37/100\n",
            " - 0s - loss: 0.0652 - acc: 0.9775 - val_loss: 0.0865 - val_acc: 0.9591\n",
            "Epoch 38/100\n",
            " - 0s - loss: 0.0580 - acc: 0.9800 - val_loss: 0.0827 - val_acc: 0.9665\n",
            "Epoch 39/100\n",
            " - 0s - loss: 0.0555 - acc: 0.9750 - val_loss: 0.0841 - val_acc: 0.9665\n",
            "Epoch 40/100\n",
            " - 0s - loss: 0.0526 - acc: 0.9800 - val_loss: 0.0808 - val_acc: 0.9703\n",
            "Epoch 41/100\n",
            " - 0s - loss: 0.0487 - acc: 0.9825 - val_loss: 0.0804 - val_acc: 0.9665\n",
            "Epoch 42/100\n",
            " - 0s - loss: 0.0435 - acc: 0.9875 - val_loss: 0.0805 - val_acc: 0.9703\n",
            "Epoch 43/100\n",
            " - 0s - loss: 0.0453 - acc: 0.9875 - val_loss: 0.0804 - val_acc: 0.9665\n",
            "Epoch 44/100\n",
            " - 0s - loss: 0.0425 - acc: 0.9850 - val_loss: 0.0822 - val_acc: 0.9665\n",
            "Epoch 45/100\n",
            " - 0s - loss: 0.0419 - acc: 0.9875 - val_loss: 0.0809 - val_acc: 0.9665\n",
            "Epoch 46/100\n",
            " - 0s - loss: 0.0395 - acc: 0.9875 - val_loss: 0.0805 - val_acc: 0.9628\n",
            "Epoch 47/100\n",
            " - 0s - loss: 0.0417 - acc: 0.9850 - val_loss: 0.0790 - val_acc: 0.9703\n",
            "Epoch 48/100\n",
            " - 0s - loss: 0.0364 - acc: 0.9875 - val_loss: 0.0800 - val_acc: 0.9665\n",
            "Epoch 49/100\n",
            " - 0s - loss: 0.0363 - acc: 0.9900 - val_loss: 0.0805 - val_acc: 0.9703\n",
            "Epoch 50/100\n",
            " - 0s - loss: 0.0359 - acc: 0.9875 - val_loss: 0.0823 - val_acc: 0.9665\n",
            "Epoch 51/100\n",
            " - 0s - loss: 0.0333 - acc: 0.9900 - val_loss: 0.0813 - val_acc: 0.9703\n",
            "Epoch 52/100\n",
            " - 0s - loss: 0.0346 - acc: 0.9900 - val_loss: 0.0788 - val_acc: 0.9703\n",
            "Epoch 53/100\n",
            " - 0s - loss: 0.0315 - acc: 0.9925 - val_loss: 0.0787 - val_acc: 0.9703\n",
            "Epoch 54/100\n",
            " - 0s - loss: 0.0317 - acc: 0.9925 - val_loss: 0.0844 - val_acc: 0.9703\n",
            "Epoch 55/100\n",
            " - 0s - loss: 0.0278 - acc: 0.9925 - val_loss: 0.0787 - val_acc: 0.9703\n",
            "Epoch 56/100\n",
            " - 0s - loss: 0.0291 - acc: 0.9950 - val_loss: 0.0776 - val_acc: 0.9703\n",
            "Epoch 57/100\n",
            " - 0s - loss: 0.0269 - acc: 0.9950 - val_loss: 0.0776 - val_acc: 0.9703\n",
            "Epoch 58/100\n",
            " - 0s - loss: 0.0290 - acc: 0.9900 - val_loss: 0.0810 - val_acc: 0.9740\n",
            "Epoch 59/100\n",
            " - 0s - loss: 0.0266 - acc: 0.9900 - val_loss: 0.0790 - val_acc: 0.9665\n",
            "Epoch 60/100\n",
            " - 0s - loss: 0.0286 - acc: 0.9925 - val_loss: 0.0814 - val_acc: 0.9740\n",
            "Epoch 61/100\n",
            " - 0s - loss: 0.0235 - acc: 0.9950 - val_loss: 0.0780 - val_acc: 0.9740\n",
            "Epoch 62/100\n",
            " - 0s - loss: 0.0249 - acc: 0.9950 - val_loss: 0.0783 - val_acc: 0.9703\n",
            "Epoch 63/100\n",
            " - 0s - loss: 0.0248 - acc: 0.9950 - val_loss: 0.0756 - val_acc: 0.9740\n",
            "Epoch 64/100\n",
            " - 0s - loss: 0.0239 - acc: 0.9950 - val_loss: 0.0742 - val_acc: 0.9740\n",
            "Epoch 65/100\n",
            " - 0s - loss: 0.0232 - acc: 0.9950 - val_loss: 0.0770 - val_acc: 0.9740\n",
            "Epoch 66/100\n",
            " - 0s - loss: 0.0213 - acc: 0.9950 - val_loss: 0.0738 - val_acc: 0.9777\n",
            "Epoch 67/100\n",
            " - 0s - loss: 0.0222 - acc: 0.9975 - val_loss: 0.0823 - val_acc: 0.9740\n",
            "Epoch 68/100\n",
            " - 0s - loss: 0.0207 - acc: 0.9950 - val_loss: 0.0751 - val_acc: 0.9740\n",
            "Epoch 69/100\n",
            " - 0s - loss: 0.0197 - acc: 0.9975 - val_loss: 0.0734 - val_acc: 0.9740\n",
            "Epoch 70/100\n",
            " - 0s - loss: 0.0197 - acc: 0.9950 - val_loss: 0.0719 - val_acc: 0.9777\n",
            "Epoch 71/100\n",
            " - 0s - loss: 0.0186 - acc: 0.9950 - val_loss: 0.0726 - val_acc: 0.9740\n",
            "Epoch 72/100\n",
            " - 0s - loss: 0.0178 - acc: 0.9975 - val_loss: 0.0746 - val_acc: 0.9740\n",
            "Epoch 73/100\n",
            " - 0s - loss: 0.0172 - acc: 0.9925 - val_loss: 0.0717 - val_acc: 0.9777\n",
            "Epoch 74/100\n",
            " - 0s - loss: 0.0216 - acc: 0.9925 - val_loss: 0.0729 - val_acc: 0.9740\n",
            "Epoch 75/100\n",
            " - 0s - loss: 0.0194 - acc: 0.9950 - val_loss: 0.0731 - val_acc: 0.9740\n",
            "Epoch 76/100\n",
            " - 0s - loss: 0.0166 - acc: 0.9950 - val_loss: 0.0722 - val_acc: 0.9740\n",
            "Epoch 77/100\n",
            " - 0s - loss: 0.0207 - acc: 0.9950 - val_loss: 0.0850 - val_acc: 0.9740\n",
            "Epoch 78/100\n",
            " - 0s - loss: 0.0167 - acc: 0.9925 - val_loss: 0.0685 - val_acc: 0.9740\n",
            "Epoch 79/100\n",
            " - 0s - loss: 0.0177 - acc: 0.9950 - val_loss: 0.0682 - val_acc: 0.9777\n",
            "Epoch 80/100\n",
            " - 0s - loss: 0.0156 - acc: 0.9975 - val_loss: 0.0679 - val_acc: 0.9777\n",
            "Epoch 81/100\n",
            " - 0s - loss: 0.0136 - acc: 0.9975 - val_loss: 0.0753 - val_acc: 0.9740\n",
            "Epoch 82/100\n",
            " - 0s - loss: 0.0172 - acc: 0.9950 - val_loss: 0.0666 - val_acc: 0.9777\n",
            "Epoch 83/100\n",
            " - 0s - loss: 0.0141 - acc: 0.9950 - val_loss: 0.0692 - val_acc: 0.9740\n",
            "Epoch 84/100\n",
            " - 0s - loss: 0.0129 - acc: 0.9975 - val_loss: 0.0718 - val_acc: 0.9740\n",
            "Epoch 85/100\n",
            " - 0s - loss: 0.0126 - acc: 0.9950 - val_loss: 0.0668 - val_acc: 0.9777\n",
            "Epoch 86/100\n",
            " - 0s - loss: 0.0135 - acc: 0.9975 - val_loss: 0.0671 - val_acc: 0.9777\n",
            "Epoch 87/100\n",
            " - 0s - loss: 0.0114 - acc: 0.9950 - val_loss: 0.0686 - val_acc: 0.9740\n",
            "Epoch 88/100\n",
            " - 0s - loss: 0.0110 - acc: 0.9975 - val_loss: 0.0655 - val_acc: 0.9777\n",
            "Epoch 89/100\n",
            " - 0s - loss: 0.0125 - acc: 0.9975 - val_loss: 0.0720 - val_acc: 0.9740\n",
            "Epoch 90/100\n",
            " - 0s - loss: 0.0118 - acc: 0.9975 - val_loss: 0.0681 - val_acc: 0.9740\n",
            "Epoch 91/100\n",
            " - 0s - loss: 0.0100 - acc: 0.9975 - val_loss: 0.0643 - val_acc: 0.9777\n",
            "Epoch 92/100\n",
            " - 0s - loss: 0.0131 - acc: 0.9975 - val_loss: 0.0798 - val_acc: 0.9740\n",
            "Epoch 93/100\n",
            " - 0s - loss: 0.0109 - acc: 0.9975 - val_loss: 0.0673 - val_acc: 0.9740\n",
            "Epoch 94/100\n",
            " - 0s - loss: 0.0093 - acc: 0.9975 - val_loss: 0.0636 - val_acc: 0.9777\n",
            "Epoch 95/100\n",
            " - 0s - loss: 0.0100 - acc: 0.9950 - val_loss: 0.0634 - val_acc: 0.9777\n",
            "Epoch 96/100\n",
            " - 0s - loss: 0.0099 - acc: 0.9975 - val_loss: 0.0690 - val_acc: 0.9740\n",
            "Epoch 97/100\n",
            " - 0s - loss: 0.0087 - acc: 1.0000 - val_loss: 0.0639 - val_acc: 0.9777\n",
            "Epoch 98/100\n",
            " - 0s - loss: 0.0077 - acc: 1.0000 - val_loss: 0.0670 - val_acc: 0.9740\n",
            "Epoch 99/100\n",
            " - 0s - loss: 0.0079 - acc: 1.0000 - val_loss: 0.0647 - val_acc: 0.9777\n",
            "Epoch 100/100\n",
            " - 0s - loss: 0.0077 - acc: 1.0000 - val_loss: 0.0635 - val_acc: 0.9777\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AntQNMTcsitc",
        "colab_type": "code",
        "outputId": "ddb081bb-8fdf-4aac-ee5b-e9fe5a2e70ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "acc_df = pd.DataFrame()\n",
        "acc_df['predict'] = test.flatten()\n",
        "acc_df.head()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>predict</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.000003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.000003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.000002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.000083</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    predict\n",
              "0  0.000003\n",
              "1  0.000006\n",
              "2  0.000003\n",
              "3  0.000002\n",
              "4  0.000083"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDUeCAD2tu3k",
        "colab_type": "code",
        "outputId": "ce0a9d55-ee4d-4fff-ad13-aefe6eedd31e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        }
      },
      "source": [
        "acc_df['labels'] = y1_test\n",
        "acc_df.head(10)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>predict</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.000003</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000006</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.000003</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.000002</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.000083</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.999999</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.999999</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.000007</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.000012</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.000933</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    predict  labels\n",
              "0  0.000003       0\n",
              "1  0.000006       0\n",
              "2  0.000003       0\n",
              "3  0.000002       0\n",
              "4  0.000083       0\n",
              "5  0.999999       1\n",
              "6  0.999999       1\n",
              "7  0.000007       0\n",
              "8  0.000012       0\n",
              "9  0.000933       0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSOEezEQt88e",
        "colab_type": "code",
        "outputId": "106926f1-8e04-4395-89c4-345aa206f340",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(\"Accuracy is: {}\\nBUDAM BUM!!\".format(1.0*sum((acc_df['labels']==1)==(acc_df['predict']>0.5))/len(acc_df['predict'])))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy is: 0.9776951672862454\n",
            "BUDAM BUM!!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnmCUpqxkzhj",
        "colab_type": "text"
      },
      "source": [
        "## Task 2. Classify all days of the week\n",
        "Train a neural network against the origial categorical label. Use 5 layers of 40,15,5 and 7 (outputs, representing probabilities for a current input to correspond to each of the weekdays) neurons over the training sample, evaluating its performance over the test sample (use 'categorical_accurary'). Report the acheived accuracy (categorical) over the test sample.\n",
        "\n",
        "First three layers use relu activation function, last one - sigmoid.\n",
        "Use loss='binary_crossentropy', optimizer='adam', 200 epochs, batch_size=20"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhIEx6zhwaM6",
        "colab_type": "code",
        "outputId": "422a9fe9-33cf-4c9b-aa0c-b643726bc391",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y=np.array(range(669))%7; y[:16]"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvVuoqTIxhYR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "015243fc-eda8-4d81-dd42-0a1dbf8c5560"
      },
      "source": [
        "'''\n",
        "y = list(y)\n",
        "for i, item in  enumerate(y):\n",
        "  size = y.count(item)\n",
        "  y[i] = item/size\n",
        "'''"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ny = list(y)\\nfor i, item in  enumerate(y):\\n  size = y.count(item)\\n  y[i] = item/size\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-4YXpPT-cAN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#yg = np_utils.to_categorical(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLLhwmNR-892",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#y = np_utils.to_categorical(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfifqB4c0-zw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y2_train=y[:400]\n",
        "y2_test=y[400:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpoHmeAM-a6v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7fdfc381-691a-4e6e-9779-49b19985b59f"
      },
      "source": [
        "'''y2_train=np_utils.to_categorical(y2_train)\n",
        "y2_test=np_utils.to_categorical(y2_test)\n",
        "'''"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'y2_train=np_utils.to_categorical(y2_train)\\ny2_test=np_utils.to_categorical(y2_test)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JN77Iga9AitV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d9019218-ffc4-4613-a21e-41891c44f60e"
      },
      "source": [
        "'''y2_test = y2_test.flatten()\n",
        "y2_train= y2_train.flatten()\n",
        "'''"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'y2_test = y2_test.flatten()\\ny2_train= y2_train.flatten()\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gT4CTUAVkzhj",
        "colab_type": "code",
        "outputId": "55e4d6f1-8eac-4f2b-aad2-187ba88fca6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "np.random.seed(1216)\n",
        "model = Sequential()\n",
        "model.add(Dense(40, activation='relu', input_dim=dim))\n",
        "model.add(Dense(15, activation='relu'))\n",
        "model.add(Dense(5, activation='relu'))\n",
        "model.add(Dense(7, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['categorical_accuracy'])\n",
        "model.fit(X_train, y2_train, validation_data=(X_test, y2_test), epochs=200, batch_size=20, verbose=2)\n",
        "scores = model.evaluate(X_test, y2_test, verbose = 0)\n",
        "test2 = model.predict(X_test)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 400 samples, validate on 269 samples\n",
            "Epoch 1/200\n",
            " - 1s - loss: 0.5294 - categorical_accuracy: 1.0000 - val_loss: 0.0261 - val_categorical_accuracy: 1.0000\n",
            "Epoch 2/200\n",
            " - 0s - loss: -9.6099e-01 - categorical_accuracy: 1.0000 - val_loss: -2.5879e+00 - val_categorical_accuracy: 1.0000\n",
            "Epoch 3/200\n",
            " - 0s - loss: -6.0884e+00 - categorical_accuracy: 1.0000 - val_loss: -1.1413e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 4/200\n",
            " - 0s - loss: -2.0704e+01 - categorical_accuracy: 1.0000 - val_loss: -3.0633e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 5/200\n",
            " - 0s - loss: -3.1675e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 6/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 7/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 8/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 9/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 10/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 11/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 12/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 13/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 14/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 15/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 16/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 17/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 18/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 19/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 20/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 21/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 22/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 23/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 24/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 25/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 26/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 27/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 28/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 29/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 30/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 31/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 32/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 33/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 34/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 35/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 36/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 37/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 38/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 39/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 40/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 41/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 42/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 43/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 44/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 45/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 46/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 47/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 48/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 49/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 50/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 51/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 52/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 53/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 54/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 55/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 56/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 57/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 58/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 59/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 60/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 61/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 62/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 63/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 64/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 65/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 66/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 67/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 68/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 69/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 70/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 71/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 72/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 73/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 74/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 75/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 76/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 77/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 78/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 79/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 80/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 81/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 82/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 83/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 84/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 85/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 86/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 87/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 88/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 89/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 90/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 91/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 92/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 93/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 94/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 95/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 96/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 97/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 98/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 99/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 100/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 101/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 102/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 103/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 104/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 105/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 106/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 107/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 108/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 109/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 110/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 111/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 112/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 113/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 114/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 115/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 116/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 117/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 118/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 119/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 120/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 121/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 122/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 123/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 124/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 125/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 126/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 127/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 128/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 129/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 130/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 131/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 132/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 133/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 134/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 135/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 136/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 137/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 138/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 139/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 140/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 141/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 142/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 143/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 144/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 145/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 146/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 147/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 148/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 149/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 150/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 151/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 152/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 153/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 154/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 155/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 156/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 157/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 158/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 159/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 160/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 161/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 162/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 163/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 164/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 165/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 166/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 167/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 168/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 169/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 170/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 171/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 172/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 173/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 174/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 175/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 176/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 177/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 178/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 179/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 180/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 181/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 182/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 183/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 184/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 185/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 186/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 187/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 188/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 189/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 190/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 191/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 192/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 193/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 194/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 195/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 196/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 197/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 198/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 199/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n",
            "Epoch 200/200\n",
            " - 0s - loss: -3.1765e+01 - categorical_accuracy: 1.0000 - val_loss: -3.1707e+01 - val_categorical_accuracy: 1.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUBQ8t8z1j6W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "b2fb0787-a9af-436f-e375-3f821c77af35"
      },
      "source": [
        "acc_df1 = pd.DataFrame()\n",
        "acc_df1['predict'] = test2.flatten()\n",
        "acc_df1.head()"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>predict</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   predict\n",
              "0      1.0\n",
              "1      1.0\n",
              "2      1.0\n",
              "3      1.0\n",
              "4      1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LsYSYaJ1ndF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "e8780510-08c3-40c4-8c1e-127898e263d9"
      },
      "source": [
        "acc_df1['labels'] = y2_test\n",
        "acc_df1.head(10)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>predict</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1.0</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1.0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1.0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   predict  labels\n",
              "0      1.0       1\n",
              "1      1.0       2\n",
              "2      1.0       3\n",
              "3      1.0       4\n",
              "4      1.0       5\n",
              "5      1.0       6\n",
              "6      1.0       0\n",
              "7      1.0       1\n",
              "8      1.0       2\n",
              "9      1.0       3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMc_m7Gx1qs1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "71a7c44e-5565-48f5-bb16-1bd871f1323a"
      },
      "source": [
        "print(\"Accuracy is: {}\\nDone for this semester!!\".format(1.0*sum((acc_df1['labels']==1)==(acc_df1['predict']>0.5))/len(acc_df1['predict'])))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy is: 0.1449814126394052\n",
            "Done for this semester!!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EUekpIQsxVG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}