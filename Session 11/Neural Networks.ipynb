{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Neural Networks",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pratikwatwani/Applied-Data-Science/blob/master/Session%2011/Neural%20Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5UVFoAmkzhH",
        "colab_type": "text"
      },
      "source": [
        "# Neural networks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OT9XSvkFkzhJ",
        "colab_type": "text"
      },
      "source": [
        "The purpose of the below is to classify days over years 2017-2018 by their corresponding mobility patterns between 10 zones in Taipei (quantified by an aggregated temporal network of subway ridership flows across the city)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtm0HTHDkzhK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#use Python 3.7\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Input, Dense\n",
        "from keras.utils import np_utils"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FW2LITZVkzhN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#read the data\n",
        "TNet=pd.read_csv('https://raw.githubusercontent.com/pratikwatwani/Applied-Data-Science/master/data/taipeiD_TNet2.csv',header=None);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ox85yef9kzhP",
        "colab_type": "code",
        "outputId": "d5e47c0f-2e33-4391-8b5e-a9dd27e41e0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        }
      },
      "source": [
        "TNet.head() \n",
        "#each row represents a 10x10 adjacency matrix of the normalized Taipei subway mobility network between 10 zones flattened into a 100x1 row corresponding to a single day\n",
        "#days start at jan-1-2017"
      ],
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>60</th>\n",
              "      <th>61</th>\n",
              "      <th>62</th>\n",
              "      <th>63</th>\n",
              "      <th>64</th>\n",
              "      <th>65</th>\n",
              "      <th>66</th>\n",
              "      <th>67</th>\n",
              "      <th>68</th>\n",
              "      <th>69</th>\n",
              "      <th>70</th>\n",
              "      <th>71</th>\n",
              "      <th>72</th>\n",
              "      <th>73</th>\n",
              "      <th>74</th>\n",
              "      <th>75</th>\n",
              "      <th>76</th>\n",
              "      <th>77</th>\n",
              "      <th>78</th>\n",
              "      <th>79</th>\n",
              "      <th>80</th>\n",
              "      <th>81</th>\n",
              "      <th>82</th>\n",
              "      <th>83</th>\n",
              "      <th>84</th>\n",
              "      <th>85</th>\n",
              "      <th>86</th>\n",
              "      <th>87</th>\n",
              "      <th>88</th>\n",
              "      <th>89</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.017943</td>\n",
              "      <td>0.005415</td>\n",
              "      <td>0.003590</td>\n",
              "      <td>0.008316</td>\n",
              "      <td>0.007859</td>\n",
              "      <td>0.012942</td>\n",
              "      <td>0.012196</td>\n",
              "      <td>0.019543</td>\n",
              "      <td>0.001196</td>\n",
              "      <td>0.003327</td>\n",
              "      <td>0.004588</td>\n",
              "      <td>0.016362</td>\n",
              "      <td>0.003059</td>\n",
              "      <td>0.006420</td>\n",
              "      <td>0.009864</td>\n",
              "      <td>0.005530</td>\n",
              "      <td>0.007357</td>\n",
              "      <td>0.017389</td>\n",
              "      <td>0.000923</td>\n",
              "      <td>0.001672</td>\n",
              "      <td>0.002640</td>\n",
              "      <td>0.002878</td>\n",
              "      <td>0.003133</td>\n",
              "      <td>0.001715</td>\n",
              "      <td>0.003610</td>\n",
              "      <td>0.001157</td>\n",
              "      <td>0.002406</td>\n",
              "      <td>0.004315</td>\n",
              "      <td>0.000550</td>\n",
              "      <td>0.001456</td>\n",
              "      <td>0.008631</td>\n",
              "      <td>0.007123</td>\n",
              "      <td>0.002301</td>\n",
              "      <td>0.010586</td>\n",
              "      <td>0.007468</td>\n",
              "      <td>0.010193</td>\n",
              "      <td>0.010568</td>\n",
              "      <td>0.020925</td>\n",
              "      <td>0.001346</td>\n",
              "      <td>0.002716</td>\n",
              "      <td>...</td>\n",
              "      <td>0.010876</td>\n",
              "      <td>0.007325</td>\n",
              "      <td>0.002859</td>\n",
              "      <td>0.009160</td>\n",
              "      <td>0.013417</td>\n",
              "      <td>0.009071</td>\n",
              "      <td>0.050107</td>\n",
              "      <td>0.043340</td>\n",
              "      <td>0.000679</td>\n",
              "      <td>0.003823</td>\n",
              "      <td>0.013696</td>\n",
              "      <td>0.014299</td>\n",
              "      <td>0.005237</td>\n",
              "      <td>0.015900</td>\n",
              "      <td>0.025870</td>\n",
              "      <td>0.021652</td>\n",
              "      <td>0.035190</td>\n",
              "      <td>0.049923</td>\n",
              "      <td>0.002971</td>\n",
              "      <td>0.009171</td>\n",
              "      <td>0.001081</td>\n",
              "      <td>0.001064</td>\n",
              "      <td>0.000710</td>\n",
              "      <td>0.001091</td>\n",
              "      <td>0.003131</td>\n",
              "      <td>0.008141</td>\n",
              "      <td>0.000839</td>\n",
              "      <td>0.004099</td>\n",
              "      <td>0.009125</td>\n",
              "      <td>0.005163</td>\n",
              "      <td>0.002529</td>\n",
              "      <td>0.001533</td>\n",
              "      <td>0.001860</td>\n",
              "      <td>0.002375</td>\n",
              "      <td>0.005408</td>\n",
              "      <td>0.008922</td>\n",
              "      <td>0.003945</td>\n",
              "      <td>0.011075</td>\n",
              "      <td>0.005073</td>\n",
              "      <td>0.012708</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.021283</td>\n",
              "      <td>0.005215</td>\n",
              "      <td>0.003530</td>\n",
              "      <td>0.009359</td>\n",
              "      <td>0.007803</td>\n",
              "      <td>0.014288</td>\n",
              "      <td>0.011185</td>\n",
              "      <td>0.019044</td>\n",
              "      <td>0.001382</td>\n",
              "      <td>0.003499</td>\n",
              "      <td>0.004859</td>\n",
              "      <td>0.016886</td>\n",
              "      <td>0.003053</td>\n",
              "      <td>0.007339</td>\n",
              "      <td>0.009820</td>\n",
              "      <td>0.005745</td>\n",
              "      <td>0.006608</td>\n",
              "      <td>0.016490</td>\n",
              "      <td>0.001023</td>\n",
              "      <td>0.001866</td>\n",
              "      <td>0.002897</td>\n",
              "      <td>0.002929</td>\n",
              "      <td>0.002973</td>\n",
              "      <td>0.001817</td>\n",
              "      <td>0.003471</td>\n",
              "      <td>0.001210</td>\n",
              "      <td>0.002197</td>\n",
              "      <td>0.004039</td>\n",
              "      <td>0.000664</td>\n",
              "      <td>0.001655</td>\n",
              "      <td>0.009672</td>\n",
              "      <td>0.007348</td>\n",
              "      <td>0.002248</td>\n",
              "      <td>0.012551</td>\n",
              "      <td>0.007475</td>\n",
              "      <td>0.011087</td>\n",
              "      <td>0.009090</td>\n",
              "      <td>0.020644</td>\n",
              "      <td>0.001560</td>\n",
              "      <td>0.003032</td>\n",
              "      <td>...</td>\n",
              "      <td>0.010139</td>\n",
              "      <td>0.006609</td>\n",
              "      <td>0.002760</td>\n",
              "      <td>0.008469</td>\n",
              "      <td>0.010956</td>\n",
              "      <td>0.009114</td>\n",
              "      <td>0.046897</td>\n",
              "      <td>0.038464</td>\n",
              "      <td>0.000647</td>\n",
              "      <td>0.003762</td>\n",
              "      <td>0.014380</td>\n",
              "      <td>0.016011</td>\n",
              "      <td>0.003765</td>\n",
              "      <td>0.017911</td>\n",
              "      <td>0.022929</td>\n",
              "      <td>0.021901</td>\n",
              "      <td>0.034270</td>\n",
              "      <td>0.040281</td>\n",
              "      <td>0.003776</td>\n",
              "      <td>0.009128</td>\n",
              "      <td>0.001298</td>\n",
              "      <td>0.001179</td>\n",
              "      <td>0.000663</td>\n",
              "      <td>0.001337</td>\n",
              "      <td>0.003490</td>\n",
              "      <td>0.008978</td>\n",
              "      <td>0.000753</td>\n",
              "      <td>0.004377</td>\n",
              "      <td>0.010360</td>\n",
              "      <td>0.005964</td>\n",
              "      <td>0.002803</td>\n",
              "      <td>0.001757</td>\n",
              "      <td>0.001783</td>\n",
              "      <td>0.002549</td>\n",
              "      <td>0.005515</td>\n",
              "      <td>0.009650</td>\n",
              "      <td>0.003596</td>\n",
              "      <td>0.009618</td>\n",
              "      <td>0.005946</td>\n",
              "      <td>0.013709</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.028988</td>\n",
              "      <td>0.006511</td>\n",
              "      <td>0.005591</td>\n",
              "      <td>0.012970</td>\n",
              "      <td>0.007816</td>\n",
              "      <td>0.015878</td>\n",
              "      <td>0.010973</td>\n",
              "      <td>0.015768</td>\n",
              "      <td>0.002252</td>\n",
              "      <td>0.005388</td>\n",
              "      <td>0.006879</td>\n",
              "      <td>0.013790</td>\n",
              "      <td>0.003706</td>\n",
              "      <td>0.009401</td>\n",
              "      <td>0.008878</td>\n",
              "      <td>0.006245</td>\n",
              "      <td>0.006937</td>\n",
              "      <td>0.013613</td>\n",
              "      <td>0.001545</td>\n",
              "      <td>0.002790</td>\n",
              "      <td>0.005575</td>\n",
              "      <td>0.003375</td>\n",
              "      <td>0.004373</td>\n",
              "      <td>0.003305</td>\n",
              "      <td>0.005246</td>\n",
              "      <td>0.001148</td>\n",
              "      <td>0.002829</td>\n",
              "      <td>0.003980</td>\n",
              "      <td>0.000803</td>\n",
              "      <td>0.002643</td>\n",
              "      <td>0.013623</td>\n",
              "      <td>0.008729</td>\n",
              "      <td>0.003722</td>\n",
              "      <td>0.015259</td>\n",
              "      <td>0.006972</td>\n",
              "      <td>0.012822</td>\n",
              "      <td>0.009648</td>\n",
              "      <td>0.017857</td>\n",
              "      <td>0.002672</td>\n",
              "      <td>0.004533</td>\n",
              "      <td>...</td>\n",
              "      <td>0.011613</td>\n",
              "      <td>0.007232</td>\n",
              "      <td>0.003313</td>\n",
              "      <td>0.009770</td>\n",
              "      <td>0.008924</td>\n",
              "      <td>0.009524</td>\n",
              "      <td>0.039863</td>\n",
              "      <td>0.029368</td>\n",
              "      <td>0.000581</td>\n",
              "      <td>0.005011</td>\n",
              "      <td>0.013882</td>\n",
              "      <td>0.013373</td>\n",
              "      <td>0.003620</td>\n",
              "      <td>0.017100</td>\n",
              "      <td>0.018839</td>\n",
              "      <td>0.018666</td>\n",
              "      <td>0.026413</td>\n",
              "      <td>0.030177</td>\n",
              "      <td>0.003673</td>\n",
              "      <td>0.008805</td>\n",
              "      <td>0.002030</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.000786</td>\n",
              "      <td>0.002192</td>\n",
              "      <td>0.004388</td>\n",
              "      <td>0.010398</td>\n",
              "      <td>0.000546</td>\n",
              "      <td>0.004129</td>\n",
              "      <td>0.011692</td>\n",
              "      <td>0.009807</td>\n",
              "      <td>0.004649</td>\n",
              "      <td>0.002555</td>\n",
              "      <td>0.002672</td>\n",
              "      <td>0.004291</td>\n",
              "      <td>0.007385</td>\n",
              "      <td>0.009558</td>\n",
              "      <td>0.004293</td>\n",
              "      <td>0.008791</td>\n",
              "      <td>0.010040</td>\n",
              "      <td>0.016301</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.029534</td>\n",
              "      <td>0.006471</td>\n",
              "      <td>0.005615</td>\n",
              "      <td>0.013017</td>\n",
              "      <td>0.007717</td>\n",
              "      <td>0.016098</td>\n",
              "      <td>0.011182</td>\n",
              "      <td>0.015815</td>\n",
              "      <td>0.002325</td>\n",
              "      <td>0.005443</td>\n",
              "      <td>0.006955</td>\n",
              "      <td>0.014044</td>\n",
              "      <td>0.003699</td>\n",
              "      <td>0.009330</td>\n",
              "      <td>0.008967</td>\n",
              "      <td>0.006290</td>\n",
              "      <td>0.007313</td>\n",
              "      <td>0.013566</td>\n",
              "      <td>0.001511</td>\n",
              "      <td>0.002833</td>\n",
              "      <td>0.005504</td>\n",
              "      <td>0.003456</td>\n",
              "      <td>0.004210</td>\n",
              "      <td>0.003239</td>\n",
              "      <td>0.005267</td>\n",
              "      <td>0.001098</td>\n",
              "      <td>0.002910</td>\n",
              "      <td>0.003914</td>\n",
              "      <td>0.000742</td>\n",
              "      <td>0.002519</td>\n",
              "      <td>0.013751</td>\n",
              "      <td>0.008552</td>\n",
              "      <td>0.003736</td>\n",
              "      <td>0.014924</td>\n",
              "      <td>0.006757</td>\n",
              "      <td>0.012755</td>\n",
              "      <td>0.009960</td>\n",
              "      <td>0.017484</td>\n",
              "      <td>0.002665</td>\n",
              "      <td>0.004498</td>\n",
              "      <td>...</td>\n",
              "      <td>0.011870</td>\n",
              "      <td>0.007473</td>\n",
              "      <td>0.003513</td>\n",
              "      <td>0.010152</td>\n",
              "      <td>0.009209</td>\n",
              "      <td>0.009930</td>\n",
              "      <td>0.041379</td>\n",
              "      <td>0.029797</td>\n",
              "      <td>0.000618</td>\n",
              "      <td>0.005187</td>\n",
              "      <td>0.013526</td>\n",
              "      <td>0.012225</td>\n",
              "      <td>0.003561</td>\n",
              "      <td>0.016417</td>\n",
              "      <td>0.018527</td>\n",
              "      <td>0.017725</td>\n",
              "      <td>0.025343</td>\n",
              "      <td>0.030699</td>\n",
              "      <td>0.003375</td>\n",
              "      <td>0.007993</td>\n",
              "      <td>0.002014</td>\n",
              "      <td>0.001469</td>\n",
              "      <td>0.000773</td>\n",
              "      <td>0.002228</td>\n",
              "      <td>0.004599</td>\n",
              "      <td>0.010936</td>\n",
              "      <td>0.000596</td>\n",
              "      <td>0.004077</td>\n",
              "      <td>0.012252</td>\n",
              "      <td>0.009988</td>\n",
              "      <td>0.004611</td>\n",
              "      <td>0.002473</td>\n",
              "      <td>0.002636</td>\n",
              "      <td>0.004195</td>\n",
              "      <td>0.007255</td>\n",
              "      <td>0.009487</td>\n",
              "      <td>0.004316</td>\n",
              "      <td>0.008729</td>\n",
              "      <td>0.010296</td>\n",
              "      <td>0.016437</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.029333</td>\n",
              "      <td>0.006525</td>\n",
              "      <td>0.005727</td>\n",
              "      <td>0.013098</td>\n",
              "      <td>0.007692</td>\n",
              "      <td>0.016358</td>\n",
              "      <td>0.011000</td>\n",
              "      <td>0.015677</td>\n",
              "      <td>0.002344</td>\n",
              "      <td>0.005527</td>\n",
              "      <td>0.006860</td>\n",
              "      <td>0.013586</td>\n",
              "      <td>0.003710</td>\n",
              "      <td>0.009243</td>\n",
              "      <td>0.008994</td>\n",
              "      <td>0.006580</td>\n",
              "      <td>0.007113</td>\n",
              "      <td>0.014127</td>\n",
              "      <td>0.001536</td>\n",
              "      <td>0.002922</td>\n",
              "      <td>0.005472</td>\n",
              "      <td>0.003366</td>\n",
              "      <td>0.004166</td>\n",
              "      <td>0.003233</td>\n",
              "      <td>0.005255</td>\n",
              "      <td>0.001152</td>\n",
              "      <td>0.002889</td>\n",
              "      <td>0.003851</td>\n",
              "      <td>0.000735</td>\n",
              "      <td>0.002525</td>\n",
              "      <td>0.013695</td>\n",
              "      <td>0.008369</td>\n",
              "      <td>0.003819</td>\n",
              "      <td>0.015103</td>\n",
              "      <td>0.006924</td>\n",
              "      <td>0.012921</td>\n",
              "      <td>0.009824</td>\n",
              "      <td>0.017778</td>\n",
              "      <td>0.002605</td>\n",
              "      <td>0.004648</td>\n",
              "      <td>...</td>\n",
              "      <td>0.011968</td>\n",
              "      <td>0.007428</td>\n",
              "      <td>0.003594</td>\n",
              "      <td>0.010037</td>\n",
              "      <td>0.009058</td>\n",
              "      <td>0.009952</td>\n",
              "      <td>0.040614</td>\n",
              "      <td>0.030371</td>\n",
              "      <td>0.000633</td>\n",
              "      <td>0.005188</td>\n",
              "      <td>0.013355</td>\n",
              "      <td>0.011994</td>\n",
              "      <td>0.003674</td>\n",
              "      <td>0.016204</td>\n",
              "      <td>0.018132</td>\n",
              "      <td>0.017880</td>\n",
              "      <td>0.025106</td>\n",
              "      <td>0.030883</td>\n",
              "      <td>0.003316</td>\n",
              "      <td>0.008219</td>\n",
              "      <td>0.002130</td>\n",
              "      <td>0.001476</td>\n",
              "      <td>0.000823</td>\n",
              "      <td>0.002186</td>\n",
              "      <td>0.004413</td>\n",
              "      <td>0.010712</td>\n",
              "      <td>0.000562</td>\n",
              "      <td>0.004160</td>\n",
              "      <td>0.011789</td>\n",
              "      <td>0.009981</td>\n",
              "      <td>0.004694</td>\n",
              "      <td>0.002515</td>\n",
              "      <td>0.002677</td>\n",
              "      <td>0.004222</td>\n",
              "      <td>0.007269</td>\n",
              "      <td>0.009921</td>\n",
              "      <td>0.004387</td>\n",
              "      <td>0.008923</td>\n",
              "      <td>0.010381</td>\n",
              "      <td>0.016914</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 100 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         0         1         2   ...        97        98        99\n",
              "0  0.017943  0.005415  0.003590  ...  0.011075  0.005073  0.012708\n",
              "1  0.021283  0.005215  0.003530  ...  0.009618  0.005946  0.013709\n",
              "2  0.028988  0.006511  0.005591  ...  0.008791  0.010040  0.016301\n",
              "3  0.029534  0.006471  0.005615  ...  0.008729  0.010296  0.016437\n",
              "4  0.029333  0.006525  0.005727  ...  0.008923  0.010381  0.016914\n",
              "\n",
              "[5 rows x 100 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 202
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYIKxV9BkzhS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#convert to an array and scale the data\n",
        "X=np.array(TNet);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHuKaNkHkzhU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X=MinMaxScaler(feature_range=(0, 1), copy=True).fit_transform(X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gu4LSECmkzhW",
        "colab_type": "code",
        "outputId": "bc028571-be55-42e1-841f-618ecf4dfaf9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X.shape"
      ],
      "execution_count": 205,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(669, 100)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 205
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ErlgDYPBkzhY",
        "colab_type": "code",
        "outputId": "5080d6b9-3f73-4770-abbe-54e5c53d604b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#define day of the week corresponding to each day of observation; 0-Sunday, 1-Monday,...,6-Saturday\n",
        "y=np.array(range(669))%7; y[:10]"
      ],
      "execution_count": 206,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 2, 3, 4, 5, 6, 0, 1, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 206
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayvjP2wjkzha",
        "colab_type": "code",
        "outputId": "26d73313-9fae-458a-9eb5-23ee97a22b1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "yc=np_utils.to_categorical(y) #get categorical binary variables isSunday, isMonday,...\n",
        "yc[:5]"
      ],
      "execution_count": 207,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 1., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 1., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 207
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSB4kHhYkzhc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_test=X[400:,:]; X_train=X[:400,:]; #split the data into training and test\n",
        "y_test=yc[400:,:]; y_train=yc[:400,:]\n",
        "dim = X_train.shape[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8qkzBH1qqoW",
        "colab_type": "code",
        "outputId": "30bfec5b-0aaa-4671-ba3f-6445c1fc9e50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "print(X_test.shape)\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": 209,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(269, 100)\n",
            "(400, 100)\n",
            "(400, 7)\n",
            "(269, 7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoVuc7aOkzhi",
        "colab_type": "text"
      },
      "source": [
        "## Task 1. Classify weekdays/weekends\n",
        "Label the rows with ones for weekends, zeros for weekdays.\n",
        "Train a neural network with 4 layers of 30,10,3 and 1 (output) neurons over the training sample against this label, evaluating its performance over the test sample. Report the acheived accuracy (categorical) over the test sample\n",
        "\n",
        "First three layers use relu activation function, last one - sigmoid.\n",
        "Use loss='binary_crossentropy', optimizer='adam', 100 epochs, batch_size=20. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_rdDQtRnbDy",
        "colab_type": "code",
        "outputId": "1b327c31-d5b9-42d4-8d86-2a68578f9fc7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        }
      },
      "source": [
        "y"
      ],
      "execution_count": 210,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0,\n",
              "       1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1,\n",
              "       2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2,\n",
              "       3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3,\n",
              "       4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4,\n",
              "       5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5,\n",
              "       6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6,\n",
              "       0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0,\n",
              "       1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1,\n",
              "       2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2,\n",
              "       3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3,\n",
              "       4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4,\n",
              "       5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5,\n",
              "       6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6,\n",
              "       0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0,\n",
              "       1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1,\n",
              "       2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2,\n",
              "       3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3,\n",
              "       4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4,\n",
              "       5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5,\n",
              "       6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6,\n",
              "       0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0,\n",
              "       1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1,\n",
              "       2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2,\n",
              "       3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3,\n",
              "       4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4,\n",
              "       5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5,\n",
              "       6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6,\n",
              "       0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0,\n",
              "       1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1,\n",
              "       2, 3, 4, 5, 6, 0, 1, 2, 3])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 210
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIzdBDqsk-9k",
        "colab_type": "code",
        "outputId": "3311fcb5-82ab-4ea9-87c0-edcb7c5f16d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        }
      },
      "source": [
        "y_c = y\n",
        "for idx, item in enumerate(y_c):\n",
        "  if ((item == 0)| (item == 6)):\n",
        "    y_c[idx] = 1\n",
        "  else:\n",
        "    y_c[idx] = 0\n",
        "\n",
        "y_c"
      ],
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,\n",
              "       0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,\n",
              "       0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
              "       0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
              "       0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
              "       0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
              "       1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n",
              "       1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,\n",
              "       0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,\n",
              "       0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
              "       0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
              "       0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
              "       0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
              "       1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n",
              "       1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,\n",
              "       0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,\n",
              "       0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
              "       0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
              "       0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
              "       0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
              "       1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n",
              "       1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,\n",
              "       0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,\n",
              "       0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
              "       0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
              "       0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
              "       0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
              "       1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n",
              "       1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,\n",
              "       0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,\n",
              "       0, 0, 0, 0, 1, 1, 0, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 211
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0W7JKnzHrZzF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y1_train=y_c[:400]\n",
        "y1_test=y_c[400:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4F6T__gq0C9",
        "colab_type": "code",
        "outputId": "3e004033-90fa-4840-9ede-9e9015762d21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(y1_train.shape)\n",
        "print(y1_test.shape)\n"
      ],
      "execution_count": 213,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(400,)\n",
            "(269,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNPlpyQQnwTy",
        "colab_type": "code",
        "outputId": "c3d35478-dc03-45cd-e5d6-f5ca928ac6bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "np.random.seed(1612)\n",
        "test = pd.DataFrame()\n",
        "model = Sequential()\n",
        "model.add(Dense(30, activation='relu', input_dim=dim))\n",
        "model.add(Dense(10, activation='relu'))\n",
        "model.add(Dense(3, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(X_train, y1_train, validation_data=(X_test, y1_test), epochs=100, batch_size=20, verbose=2)\n",
        "test = model.predict(X_test)"
      ],
      "execution_count": 214,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 400 samples, validate on 269 samples\n",
            "Epoch 1/100\n",
            " - 2s - loss: 0.4269 - acc: 0.7125 - val_loss: 0.3396 - val_acc: 0.7175\n",
            "Epoch 2/100\n",
            " - 0s - loss: 0.3198 - acc: 0.7125 - val_loss: 0.2875 - val_acc: 0.7175\n",
            "Epoch 3/100\n",
            " - 0s - loss: 0.2802 - acc: 0.7350 - val_loss: 0.2579 - val_acc: 0.7918\n",
            "Epoch 4/100\n",
            " - 0s - loss: 0.2409 - acc: 0.9300 - val_loss: 0.2130 - val_acc: 0.9554\n",
            "Epoch 5/100\n",
            " - 0s - loss: 0.2046 - acc: 0.9525 - val_loss: 0.1837 - val_acc: 0.9554\n",
            "Epoch 6/100\n",
            " - 0s - loss: 0.1770 - acc: 0.9525 - val_loss: 0.1608 - val_acc: 0.9554\n",
            "Epoch 7/100\n",
            " - 0s - loss: 0.1551 - acc: 0.9525 - val_loss: 0.1446 - val_acc: 0.9554\n",
            "Epoch 8/100\n",
            " - 0s - loss: 0.1413 - acc: 0.9525 - val_loss: 0.1355 - val_acc: 0.9554\n",
            "Epoch 9/100\n",
            " - 0s - loss: 0.1366 - acc: 0.9525 - val_loss: 0.1303 - val_acc: 0.9554\n",
            "Epoch 10/100\n",
            " - 0s - loss: 0.1314 - acc: 0.9525 - val_loss: 0.1271 - val_acc: 0.9554\n",
            "Epoch 11/100\n",
            " - 0s - loss: 0.1331 - acc: 0.9550 - val_loss: 0.1272 - val_acc: 0.9517\n",
            "Epoch 12/100\n",
            " - 0s - loss: 0.1259 - acc: 0.9525 - val_loss: 0.1232 - val_acc: 0.9554\n",
            "Epoch 13/100\n",
            " - 0s - loss: 0.1219 - acc: 0.9525 - val_loss: 0.1219 - val_acc: 0.9554\n",
            "Epoch 14/100\n",
            " - 0s - loss: 0.1185 - acc: 0.9525 - val_loss: 0.1194 - val_acc: 0.9554\n",
            "Epoch 15/100\n",
            " - 0s - loss: 0.1174 - acc: 0.9525 - val_loss: 0.1184 - val_acc: 0.9554\n",
            "Epoch 16/100\n",
            " - 0s - loss: 0.1200 - acc: 0.9550 - val_loss: 0.1160 - val_acc: 0.9554\n",
            "Epoch 17/100\n",
            " - 0s - loss: 0.1145 - acc: 0.9550 - val_loss: 0.1158 - val_acc: 0.9554\n",
            "Epoch 18/100\n",
            " - 0s - loss: 0.1072 - acc: 0.9550 - val_loss: 0.1123 - val_acc: 0.9554\n",
            "Epoch 19/100\n",
            " - 0s - loss: 0.1080 - acc: 0.9500 - val_loss: 0.1102 - val_acc: 0.9554\n",
            "Epoch 20/100\n",
            " - 0s - loss: 0.1050 - acc: 0.9550 - val_loss: 0.1087 - val_acc: 0.9554\n",
            "Epoch 21/100\n",
            " - 0s - loss: 0.1036 - acc: 0.9550 - val_loss: 0.1064 - val_acc: 0.9554\n",
            "Epoch 22/100\n",
            " - 0s - loss: 0.0970 - acc: 0.9575 - val_loss: 0.1052 - val_acc: 0.9591\n",
            "Epoch 23/100\n",
            " - 0s - loss: 0.0953 - acc: 0.9575 - val_loss: 0.1052 - val_acc: 0.9554\n",
            "Epoch 24/100\n",
            " - 0s - loss: 0.0911 - acc: 0.9575 - val_loss: 0.1007 - val_acc: 0.9628\n",
            "Epoch 25/100\n",
            " - 0s - loss: 0.0887 - acc: 0.9600 - val_loss: 0.1000 - val_acc: 0.9628\n",
            "Epoch 26/100\n",
            " - 0s - loss: 0.0848 - acc: 0.9600 - val_loss: 0.0968 - val_acc: 0.9628\n",
            "Epoch 27/100\n",
            " - 0s - loss: 0.0825 - acc: 0.9600 - val_loss: 0.0975 - val_acc: 0.9628\n",
            "Epoch 28/100\n",
            " - 0s - loss: 0.0816 - acc: 0.9575 - val_loss: 0.0956 - val_acc: 0.9628\n",
            "Epoch 29/100\n",
            " - 0s - loss: 0.0801 - acc: 0.9650 - val_loss: 0.0937 - val_acc: 0.9628\n",
            "Epoch 30/100\n",
            " - 0s - loss: 0.0773 - acc: 0.9600 - val_loss: 0.0935 - val_acc: 0.9628\n",
            "Epoch 31/100\n",
            " - 0s - loss: 0.0731 - acc: 0.9650 - val_loss: 0.0911 - val_acc: 0.9591\n",
            "Epoch 32/100\n",
            " - 0s - loss: 0.0711 - acc: 0.9675 - val_loss: 0.0906 - val_acc: 0.9591\n",
            "Epoch 33/100\n",
            " - 0s - loss: 0.0679 - acc: 0.9700 - val_loss: 0.0867 - val_acc: 0.9554\n",
            "Epoch 34/100\n",
            " - 0s - loss: 0.0609 - acc: 0.9725 - val_loss: 0.0869 - val_acc: 0.9665\n",
            "Epoch 35/100\n",
            " - 0s - loss: 0.0569 - acc: 0.9800 - val_loss: 0.0842 - val_acc: 0.9665\n",
            "Epoch 36/100\n",
            " - 0s - loss: 0.0539 - acc: 0.9875 - val_loss: 0.0905 - val_acc: 0.9591\n",
            "Epoch 37/100\n",
            " - 0s - loss: 0.0546 - acc: 0.9875 - val_loss: 0.0844 - val_acc: 0.9665\n",
            "Epoch 38/100\n",
            " - 0s - loss: 0.0509 - acc: 0.9800 - val_loss: 0.0828 - val_acc: 0.9665\n",
            "Epoch 39/100\n",
            " - 0s - loss: 0.0498 - acc: 0.9800 - val_loss: 0.0849 - val_acc: 0.9665\n",
            "Epoch 40/100\n",
            " - 0s - loss: 0.0477 - acc: 0.9825 - val_loss: 0.0815 - val_acc: 0.9665\n",
            "Epoch 41/100\n",
            " - 0s - loss: 0.0447 - acc: 0.9825 - val_loss: 0.0810 - val_acc: 0.9665\n",
            "Epoch 42/100\n",
            " - 0s - loss: 0.0401 - acc: 0.9875 - val_loss: 0.0810 - val_acc: 0.9665\n",
            "Epoch 43/100\n",
            " - 0s - loss: 0.0432 - acc: 0.9875 - val_loss: 0.0817 - val_acc: 0.9665\n",
            "Epoch 44/100\n",
            " - 0s - loss: 0.0407 - acc: 0.9875 - val_loss: 0.0851 - val_acc: 0.9665\n",
            "Epoch 45/100\n",
            " - 0s - loss: 0.0407 - acc: 0.9875 - val_loss: 0.0822 - val_acc: 0.9665\n",
            "Epoch 46/100\n",
            " - 0s - loss: 0.0380 - acc: 0.9900 - val_loss: 0.0827 - val_acc: 0.9628\n",
            "Epoch 47/100\n",
            " - 0s - loss: 0.0394 - acc: 0.9850 - val_loss: 0.0806 - val_acc: 0.9703\n",
            "Epoch 48/100\n",
            " - 0s - loss: 0.0348 - acc: 0.9900 - val_loss: 0.0819 - val_acc: 0.9665\n",
            "Epoch 49/100\n",
            " - 0s - loss: 0.0348 - acc: 0.9900 - val_loss: 0.0816 - val_acc: 0.9740\n",
            "Epoch 50/100\n",
            " - 0s - loss: 0.0340 - acc: 0.9875 - val_loss: 0.0824 - val_acc: 0.9665\n",
            "Epoch 51/100\n",
            " - 0s - loss: 0.0316 - acc: 0.9925 - val_loss: 0.0823 - val_acc: 0.9703\n",
            "Epoch 52/100\n",
            " - 0s - loss: 0.0331 - acc: 0.9925 - val_loss: 0.0798 - val_acc: 0.9703\n",
            "Epoch 53/100\n",
            " - 0s - loss: 0.0302 - acc: 0.9950 - val_loss: 0.0797 - val_acc: 0.9703\n",
            "Epoch 54/100\n",
            " - 0s - loss: 0.0306 - acc: 0.9950 - val_loss: 0.0856 - val_acc: 0.9703\n",
            "Epoch 55/100\n",
            " - 0s - loss: 0.0271 - acc: 0.9925 - val_loss: 0.0806 - val_acc: 0.9703\n",
            "Epoch 56/100\n",
            " - 0s - loss: 0.0283 - acc: 0.9950 - val_loss: 0.0795 - val_acc: 0.9703\n",
            "Epoch 57/100\n",
            " - 0s - loss: 0.0263 - acc: 0.9975 - val_loss: 0.0805 - val_acc: 0.9703\n",
            "Epoch 58/100\n",
            " - 0s - loss: 0.0282 - acc: 0.9900 - val_loss: 0.0835 - val_acc: 0.9740\n",
            "Epoch 59/100\n",
            " - 0s - loss: 0.0261 - acc: 0.9900 - val_loss: 0.0805 - val_acc: 0.9740\n",
            "Epoch 60/100\n",
            " - 0s - loss: 0.0276 - acc: 0.9925 - val_loss: 0.0828 - val_acc: 0.9740\n",
            "Epoch 61/100\n",
            " - 0s - loss: 0.0232 - acc: 0.9950 - val_loss: 0.0789 - val_acc: 0.9740\n",
            "Epoch 62/100\n",
            " - 0s - loss: 0.0245 - acc: 0.9950 - val_loss: 0.0808 - val_acc: 0.9740\n",
            "Epoch 63/100\n",
            " - 0s - loss: 0.0244 - acc: 0.9950 - val_loss: 0.0770 - val_acc: 0.9740\n",
            "Epoch 64/100\n",
            " - 0s - loss: 0.0231 - acc: 0.9975 - val_loss: 0.0758 - val_acc: 0.9777\n",
            "Epoch 65/100\n",
            " - 0s - loss: 0.0225 - acc: 0.9950 - val_loss: 0.0789 - val_acc: 0.9740\n",
            "Epoch 66/100\n",
            " - 0s - loss: 0.0209 - acc: 0.9950 - val_loss: 0.0754 - val_acc: 0.9777\n",
            "Epoch 67/100\n",
            " - 0s - loss: 0.0222 - acc: 0.9975 - val_loss: 0.0846 - val_acc: 0.9740\n",
            "Epoch 68/100\n",
            " - 0s - loss: 0.0206 - acc: 0.9950 - val_loss: 0.0768 - val_acc: 0.9740\n",
            "Epoch 69/100\n",
            " - 0s - loss: 0.0197 - acc: 0.9975 - val_loss: 0.0752 - val_acc: 0.9740\n",
            "Epoch 70/100\n",
            " - 0s - loss: 0.0196 - acc: 0.9975 - val_loss: 0.0737 - val_acc: 0.9777\n",
            "Epoch 71/100\n",
            " - 0s - loss: 0.0185 - acc: 0.9950 - val_loss: 0.0751 - val_acc: 0.9740\n",
            "Epoch 72/100\n",
            " - 0s - loss: 0.0178 - acc: 0.9975 - val_loss: 0.0760 - val_acc: 0.9740\n",
            "Epoch 73/100\n",
            " - 0s - loss: 0.0174 - acc: 0.9950 - val_loss: 0.0733 - val_acc: 0.9777\n",
            "Epoch 74/100\n",
            " - 0s - loss: 0.0216 - acc: 0.9925 - val_loss: 0.0748 - val_acc: 0.9740\n",
            "Epoch 75/100\n",
            " - 0s - loss: 0.0194 - acc: 0.9950 - val_loss: 0.0751 - val_acc: 0.9740\n",
            "Epoch 76/100\n",
            " - 0s - loss: 0.0166 - acc: 0.9975 - val_loss: 0.0744 - val_acc: 0.9740\n",
            "Epoch 77/100\n",
            " - 0s - loss: 0.0204 - acc: 0.9950 - val_loss: 0.0855 - val_acc: 0.9740\n",
            "Epoch 78/100\n",
            " - 0s - loss: 0.0167 - acc: 0.9925 - val_loss: 0.0706 - val_acc: 0.9740\n",
            "Epoch 79/100\n",
            " - 0s - loss: 0.0179 - acc: 0.9975 - val_loss: 0.0707 - val_acc: 0.9777\n",
            "Epoch 80/100\n",
            " - 0s - loss: 0.0155 - acc: 0.9975 - val_loss: 0.0703 - val_acc: 0.9777\n",
            "Epoch 81/100\n",
            " - 0s - loss: 0.0138 - acc: 0.9975 - val_loss: 0.0774 - val_acc: 0.9740\n",
            "Epoch 82/100\n",
            " - 0s - loss: 0.0171 - acc: 0.9950 - val_loss: 0.0690 - val_acc: 0.9777\n",
            "Epoch 83/100\n",
            " - 0s - loss: 0.0140 - acc: 0.9975 - val_loss: 0.0718 - val_acc: 0.9740\n",
            "Epoch 84/100\n",
            " - 0s - loss: 0.0131 - acc: 0.9975 - val_loss: 0.0744 - val_acc: 0.9740\n",
            "Epoch 85/100\n",
            " - 0s - loss: 0.0127 - acc: 0.9950 - val_loss: 0.0693 - val_acc: 0.9777\n",
            "Epoch 86/100\n",
            " - 0s - loss: 0.0139 - acc: 0.9975 - val_loss: 0.0698 - val_acc: 0.9777\n",
            "Epoch 87/100\n",
            " - 0s - loss: 0.0114 - acc: 0.9975 - val_loss: 0.0715 - val_acc: 0.9740\n",
            "Epoch 88/100\n",
            " - 0s - loss: 0.0109 - acc: 0.9975 - val_loss: 0.0683 - val_acc: 0.9777\n",
            "Epoch 89/100\n",
            " - 0s - loss: 0.0123 - acc: 0.9975 - val_loss: 0.0741 - val_acc: 0.9740\n",
            "Epoch 90/100\n",
            " - 0s - loss: 0.0118 - acc: 0.9975 - val_loss: 0.0715 - val_acc: 0.9740\n",
            "Epoch 91/100\n",
            " - 0s - loss: 0.0100 - acc: 0.9975 - val_loss: 0.0672 - val_acc: 0.9777\n",
            "Epoch 92/100\n",
            " - 0s - loss: 0.0126 - acc: 0.9975 - val_loss: 0.0807 - val_acc: 0.9740\n",
            "Epoch 93/100\n",
            " - 0s - loss: 0.0108 - acc: 0.9975 - val_loss: 0.0702 - val_acc: 0.9740\n",
            "Epoch 94/100\n",
            " - 0s - loss: 0.0093 - acc: 0.9975 - val_loss: 0.0665 - val_acc: 0.9777\n",
            "Epoch 95/100\n",
            " - 0s - loss: 0.0099 - acc: 0.9975 - val_loss: 0.0660 - val_acc: 0.9777\n",
            "Epoch 96/100\n",
            " - 0s - loss: 0.0100 - acc: 0.9975 - val_loss: 0.0716 - val_acc: 0.9740\n",
            "Epoch 97/100\n",
            " - 0s - loss: 0.0088 - acc: 0.9975 - val_loss: 0.0664 - val_acc: 0.9777\n",
            "Epoch 98/100\n",
            " - 0s - loss: 0.0078 - acc: 1.0000 - val_loss: 0.0693 - val_acc: 0.9740\n",
            "Epoch 99/100\n",
            " - 0s - loss: 0.0078 - acc: 1.0000 - val_loss: 0.0678 - val_acc: 0.9777\n",
            "Epoch 100/100\n",
            " - 0s - loss: 0.0076 - acc: 1.0000 - val_loss: 0.0662 - val_acc: 0.9777\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AntQNMTcsitc",
        "colab_type": "code",
        "outputId": "59ac9e0e-5fe1-4d72-eb73-865d9eebbe3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "acc_df = pd.DataFrame()\n",
        "acc_df['predict'] = test.flatten()\n",
        "acc_df.head()"
      ],
      "execution_count": 215,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>predict</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.000001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.000001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.000001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.000028</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    predict\n",
              "0  0.000001\n",
              "1  0.000003\n",
              "2  0.000001\n",
              "3  0.000001\n",
              "4  0.000028"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 215
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDUeCAD2tu3k",
        "colab_type": "code",
        "outputId": "333f8f2e-1ad5-468f-9e38-60bc0f9b0a60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        }
      },
      "source": [
        "acc_df['labels'] = y1_test\n",
        "acc_df.head(10)"
      ],
      "execution_count": 216,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>predict</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.000001</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000003</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.000001</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.000001</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.000028</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.999996</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.999995</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.000003</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.000007</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.000703</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    predict  labels\n",
              "0  0.000001       0\n",
              "1  0.000003       0\n",
              "2  0.000001       0\n",
              "3  0.000001       0\n",
              "4  0.000028       0\n",
              "5  0.999996       1\n",
              "6  0.999995       1\n",
              "7  0.000003       0\n",
              "8  0.000007       0\n",
              "9  0.000703       0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 216
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSOEezEQt88e",
        "colab_type": "code",
        "outputId": "f9c44fce-494c-47c5-92a3-ba2dcbb150bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(\"Accuracy is: {}\\nBUDAM BUM!!\".format(1.0*sum((acc_df['labels']==1)==(acc_df['predict']>0.5))/len(acc_df['predict'])))"
      ],
      "execution_count": 217,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy is: 0.9776951672862454\n",
            "BUDAM BUM!!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnmCUpqxkzhj",
        "colab_type": "text"
      },
      "source": [
        "## Task 2. Classify all days of the week\n",
        "Train a neural network against the origial categorical label. Use 5 layers of 40,15,5 and 7 (outputs, representing probabilities for a current input to correspond to each of the weekdays) neurons over the training sample, evaluating its performance over the test sample (use 'categorical_accurary'). Report the acheived accuracy (categorical) over the test sample.\n",
        "\n",
        "First three layers use relu activation function, last one - sigmoid.\n",
        "Use loss='binary_crossentropy', optimizer='adam', 200 epochs, batch_size=20"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhIEx6zhwaM6",
        "colab_type": "code",
        "outputId": "7ca907ce-c418-4a91-e05b-2b0cfad2aea2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y=np.array(range(669))%7; y[:16]"
      ],
      "execution_count": 235,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 235
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvVuoqTIxhYR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''y = list(y)\n",
        "for i, item in  enumerate(y):\n",
        "  size = y.count(item)\n",
        "  y[i] = item/size\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-4YXpPT-cAN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#yg = np_utils.to_categorical(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLLhwmNR-892",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = np_utils.to_categorical(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfifqB4c0-zw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y2_train=y[:400]\n",
        "y2_test=y[400:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpoHmeAM-a6v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''y2_train=np_utils.to_categorical(y2_train)\n",
        "y2_test=np_utils.to_categorical(y2_test)\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JN77Iga9AitV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''y2_test = y2_test.flatten()\n",
        "y2_train= y2_train.flatten()\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gT4CTUAVkzhj",
        "colab_type": "code",
        "outputId": "9349f341-cfc1-4c8e-ec25-ab881e58cb00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "np.random.seed(1216)\n",
        "model = Sequential()\n",
        "model.add(Dense(40, activation='relu', input_dim=dim))\n",
        "model.add(Dense(15, activation='relu'))\n",
        "model.add(Dense(5, activation='relu'))\n",
        "model.add(Dense(7, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['categorical_accuracy'])\n",
        "model.fit(X_train, y2_train, validation_data=(X_test, y2_test), epochs=200, batch_size=20, verbose=2)\n",
        "test2 = model.predict(X_test)"
      ],
      "execution_count": 234,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-234-32720443a08f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sigmoid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'categorical_accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my2_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my2_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mtest2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1087\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1088\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1089\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1090\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1091\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0;31m# Check that all arrays have the same length.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcheck_array_lengths\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m                 \u001b[0mcheck_array_length_consistency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_graph_network\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m                 \u001b[0;31m# Additional checks to avoid users mistakenly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mcheck_array_length_consistency\u001b[0;34m(inputs, targets, weights)\u001b[0m\n\u001b[1;32m    238\u001b[0m                          \u001b[0;34m'the same number of samples as target arrays. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m                          \u001b[0;34m'Found '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' input samples '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m                          'and ' + str(list(set_y)[0]) + ' target samples.')\n\u001b[0m\u001b[1;32m    241\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_w\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m         raise ValueError('All sample_weight arrays should have '\n",
            "\u001b[0;31mValueError\u001b[0m: Input arrays should have the same number of samples as target arrays. Found 400 input samples and 2800 target samples."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUBQ8t8z1j6W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "acc_df1 = pd.DataFrame()\n",
        "acc_df1['predict'] = test2.flatten()\n",
        "acc_df1.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LsYSYaJ1ndF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "acc_df1['labels'] = y2_test\n",
        "acc_df1.head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMc_m7Gx1qs1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Accuracy is: {}\\nDone for this semester!!\".format(1.0*sum((acc_df1['labels']==1)==(acc_df1['predict']>0.5))/len(acc_df1['predict'])))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}