{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Neural Networks",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pratikwatwani/Applied-Data-Science/blob/master/Session%2011/Neural%20Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5UVFoAmkzhH",
        "colab_type": "text"
      },
      "source": [
        "# Homework 7_Session 11. Neural networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OT9XSvkFkzhJ",
        "colab_type": "text"
      },
      "source": [
        "The purpose of the below is to classify days over years 2017-2018 by their corresponding mobility patterns between 10 zones in Taipei (quantified by an aggregated temporal network of subway ridership flows across the city)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtm0HTHDkzhK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "outputId": "ba55cd6f-c411-4af5-8852-db300eaa5123"
      },
      "source": [
        "#use Python 3.7\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Input, Dense\n",
        "from keras.utils import np_utils"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FW2LITZVkzhN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#read the data\n",
        "TNet=pd.read_csv('https://raw.githubusercontent.com/pratikwatwani/Applied-Data-Science/master/data/taipeiD_TNet2.csv',header=None);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ox85yef9kzhP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "e483a9ac-7631-4ce9-e06c-edb650148d85"
      },
      "source": [
        "TNet.head() \n",
        "#each row represents a 10x10 adjacency matrix of the normalized Taipei subway mobility network between 10 zones flattened into a 100x1 row corresponding to a single day\n",
        "#days start at jan-1-2017"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>60</th>\n",
              "      <th>61</th>\n",
              "      <th>62</th>\n",
              "      <th>63</th>\n",
              "      <th>64</th>\n",
              "      <th>65</th>\n",
              "      <th>66</th>\n",
              "      <th>67</th>\n",
              "      <th>68</th>\n",
              "      <th>69</th>\n",
              "      <th>70</th>\n",
              "      <th>71</th>\n",
              "      <th>72</th>\n",
              "      <th>73</th>\n",
              "      <th>74</th>\n",
              "      <th>75</th>\n",
              "      <th>76</th>\n",
              "      <th>77</th>\n",
              "      <th>78</th>\n",
              "      <th>79</th>\n",
              "      <th>80</th>\n",
              "      <th>81</th>\n",
              "      <th>82</th>\n",
              "      <th>83</th>\n",
              "      <th>84</th>\n",
              "      <th>85</th>\n",
              "      <th>86</th>\n",
              "      <th>87</th>\n",
              "      <th>88</th>\n",
              "      <th>89</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.017943</td>\n",
              "      <td>0.005415</td>\n",
              "      <td>0.003590</td>\n",
              "      <td>0.008316</td>\n",
              "      <td>0.007859</td>\n",
              "      <td>0.012942</td>\n",
              "      <td>0.012196</td>\n",
              "      <td>0.019543</td>\n",
              "      <td>0.001196</td>\n",
              "      <td>0.003327</td>\n",
              "      <td>0.004588</td>\n",
              "      <td>0.016362</td>\n",
              "      <td>0.003059</td>\n",
              "      <td>0.006420</td>\n",
              "      <td>0.009864</td>\n",
              "      <td>0.005530</td>\n",
              "      <td>0.007357</td>\n",
              "      <td>0.017389</td>\n",
              "      <td>0.000923</td>\n",
              "      <td>0.001672</td>\n",
              "      <td>0.002640</td>\n",
              "      <td>0.002878</td>\n",
              "      <td>0.003133</td>\n",
              "      <td>0.001715</td>\n",
              "      <td>0.003610</td>\n",
              "      <td>0.001157</td>\n",
              "      <td>0.002406</td>\n",
              "      <td>0.004315</td>\n",
              "      <td>0.000550</td>\n",
              "      <td>0.001456</td>\n",
              "      <td>0.008631</td>\n",
              "      <td>0.007123</td>\n",
              "      <td>0.002301</td>\n",
              "      <td>0.010586</td>\n",
              "      <td>0.007468</td>\n",
              "      <td>0.010193</td>\n",
              "      <td>0.010568</td>\n",
              "      <td>0.020925</td>\n",
              "      <td>0.001346</td>\n",
              "      <td>0.002716</td>\n",
              "      <td>...</td>\n",
              "      <td>0.010876</td>\n",
              "      <td>0.007325</td>\n",
              "      <td>0.002859</td>\n",
              "      <td>0.009160</td>\n",
              "      <td>0.013417</td>\n",
              "      <td>0.009071</td>\n",
              "      <td>0.050107</td>\n",
              "      <td>0.043340</td>\n",
              "      <td>0.000679</td>\n",
              "      <td>0.003823</td>\n",
              "      <td>0.013696</td>\n",
              "      <td>0.014299</td>\n",
              "      <td>0.005237</td>\n",
              "      <td>0.015900</td>\n",
              "      <td>0.025870</td>\n",
              "      <td>0.021652</td>\n",
              "      <td>0.035190</td>\n",
              "      <td>0.049923</td>\n",
              "      <td>0.002971</td>\n",
              "      <td>0.009171</td>\n",
              "      <td>0.001081</td>\n",
              "      <td>0.001064</td>\n",
              "      <td>0.000710</td>\n",
              "      <td>0.001091</td>\n",
              "      <td>0.003131</td>\n",
              "      <td>0.008141</td>\n",
              "      <td>0.000839</td>\n",
              "      <td>0.004099</td>\n",
              "      <td>0.009125</td>\n",
              "      <td>0.005163</td>\n",
              "      <td>0.002529</td>\n",
              "      <td>0.001533</td>\n",
              "      <td>0.001860</td>\n",
              "      <td>0.002375</td>\n",
              "      <td>0.005408</td>\n",
              "      <td>0.008922</td>\n",
              "      <td>0.003945</td>\n",
              "      <td>0.011075</td>\n",
              "      <td>0.005073</td>\n",
              "      <td>0.012708</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.021283</td>\n",
              "      <td>0.005215</td>\n",
              "      <td>0.003530</td>\n",
              "      <td>0.009359</td>\n",
              "      <td>0.007803</td>\n",
              "      <td>0.014288</td>\n",
              "      <td>0.011185</td>\n",
              "      <td>0.019044</td>\n",
              "      <td>0.001382</td>\n",
              "      <td>0.003499</td>\n",
              "      <td>0.004859</td>\n",
              "      <td>0.016886</td>\n",
              "      <td>0.003053</td>\n",
              "      <td>0.007339</td>\n",
              "      <td>0.009820</td>\n",
              "      <td>0.005745</td>\n",
              "      <td>0.006608</td>\n",
              "      <td>0.016490</td>\n",
              "      <td>0.001023</td>\n",
              "      <td>0.001866</td>\n",
              "      <td>0.002897</td>\n",
              "      <td>0.002929</td>\n",
              "      <td>0.002973</td>\n",
              "      <td>0.001817</td>\n",
              "      <td>0.003471</td>\n",
              "      <td>0.001210</td>\n",
              "      <td>0.002197</td>\n",
              "      <td>0.004039</td>\n",
              "      <td>0.000664</td>\n",
              "      <td>0.001655</td>\n",
              "      <td>0.009672</td>\n",
              "      <td>0.007348</td>\n",
              "      <td>0.002248</td>\n",
              "      <td>0.012551</td>\n",
              "      <td>0.007475</td>\n",
              "      <td>0.011087</td>\n",
              "      <td>0.009090</td>\n",
              "      <td>0.020644</td>\n",
              "      <td>0.001560</td>\n",
              "      <td>0.003032</td>\n",
              "      <td>...</td>\n",
              "      <td>0.010139</td>\n",
              "      <td>0.006609</td>\n",
              "      <td>0.002760</td>\n",
              "      <td>0.008469</td>\n",
              "      <td>0.010956</td>\n",
              "      <td>0.009114</td>\n",
              "      <td>0.046897</td>\n",
              "      <td>0.038464</td>\n",
              "      <td>0.000647</td>\n",
              "      <td>0.003762</td>\n",
              "      <td>0.014380</td>\n",
              "      <td>0.016011</td>\n",
              "      <td>0.003765</td>\n",
              "      <td>0.017911</td>\n",
              "      <td>0.022929</td>\n",
              "      <td>0.021901</td>\n",
              "      <td>0.034270</td>\n",
              "      <td>0.040281</td>\n",
              "      <td>0.003776</td>\n",
              "      <td>0.009128</td>\n",
              "      <td>0.001298</td>\n",
              "      <td>0.001179</td>\n",
              "      <td>0.000663</td>\n",
              "      <td>0.001337</td>\n",
              "      <td>0.003490</td>\n",
              "      <td>0.008978</td>\n",
              "      <td>0.000753</td>\n",
              "      <td>0.004377</td>\n",
              "      <td>0.010360</td>\n",
              "      <td>0.005964</td>\n",
              "      <td>0.002803</td>\n",
              "      <td>0.001757</td>\n",
              "      <td>0.001783</td>\n",
              "      <td>0.002549</td>\n",
              "      <td>0.005515</td>\n",
              "      <td>0.009650</td>\n",
              "      <td>0.003596</td>\n",
              "      <td>0.009618</td>\n",
              "      <td>0.005946</td>\n",
              "      <td>0.013709</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.028988</td>\n",
              "      <td>0.006511</td>\n",
              "      <td>0.005591</td>\n",
              "      <td>0.012970</td>\n",
              "      <td>0.007816</td>\n",
              "      <td>0.015878</td>\n",
              "      <td>0.010973</td>\n",
              "      <td>0.015768</td>\n",
              "      <td>0.002252</td>\n",
              "      <td>0.005388</td>\n",
              "      <td>0.006879</td>\n",
              "      <td>0.013790</td>\n",
              "      <td>0.003706</td>\n",
              "      <td>0.009401</td>\n",
              "      <td>0.008878</td>\n",
              "      <td>0.006245</td>\n",
              "      <td>0.006937</td>\n",
              "      <td>0.013613</td>\n",
              "      <td>0.001545</td>\n",
              "      <td>0.002790</td>\n",
              "      <td>0.005575</td>\n",
              "      <td>0.003375</td>\n",
              "      <td>0.004373</td>\n",
              "      <td>0.003305</td>\n",
              "      <td>0.005246</td>\n",
              "      <td>0.001148</td>\n",
              "      <td>0.002829</td>\n",
              "      <td>0.003980</td>\n",
              "      <td>0.000803</td>\n",
              "      <td>0.002643</td>\n",
              "      <td>0.013623</td>\n",
              "      <td>0.008729</td>\n",
              "      <td>0.003722</td>\n",
              "      <td>0.015259</td>\n",
              "      <td>0.006972</td>\n",
              "      <td>0.012822</td>\n",
              "      <td>0.009648</td>\n",
              "      <td>0.017857</td>\n",
              "      <td>0.002672</td>\n",
              "      <td>0.004533</td>\n",
              "      <td>...</td>\n",
              "      <td>0.011613</td>\n",
              "      <td>0.007232</td>\n",
              "      <td>0.003313</td>\n",
              "      <td>0.009770</td>\n",
              "      <td>0.008924</td>\n",
              "      <td>0.009524</td>\n",
              "      <td>0.039863</td>\n",
              "      <td>0.029368</td>\n",
              "      <td>0.000581</td>\n",
              "      <td>0.005011</td>\n",
              "      <td>0.013882</td>\n",
              "      <td>0.013373</td>\n",
              "      <td>0.003620</td>\n",
              "      <td>0.017100</td>\n",
              "      <td>0.018839</td>\n",
              "      <td>0.018666</td>\n",
              "      <td>0.026413</td>\n",
              "      <td>0.030177</td>\n",
              "      <td>0.003673</td>\n",
              "      <td>0.008805</td>\n",
              "      <td>0.002030</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.000786</td>\n",
              "      <td>0.002192</td>\n",
              "      <td>0.004388</td>\n",
              "      <td>0.010398</td>\n",
              "      <td>0.000546</td>\n",
              "      <td>0.004129</td>\n",
              "      <td>0.011692</td>\n",
              "      <td>0.009807</td>\n",
              "      <td>0.004649</td>\n",
              "      <td>0.002555</td>\n",
              "      <td>0.002672</td>\n",
              "      <td>0.004291</td>\n",
              "      <td>0.007385</td>\n",
              "      <td>0.009558</td>\n",
              "      <td>0.004293</td>\n",
              "      <td>0.008791</td>\n",
              "      <td>0.010040</td>\n",
              "      <td>0.016301</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.029534</td>\n",
              "      <td>0.006471</td>\n",
              "      <td>0.005615</td>\n",
              "      <td>0.013017</td>\n",
              "      <td>0.007717</td>\n",
              "      <td>0.016098</td>\n",
              "      <td>0.011182</td>\n",
              "      <td>0.015815</td>\n",
              "      <td>0.002325</td>\n",
              "      <td>0.005443</td>\n",
              "      <td>0.006955</td>\n",
              "      <td>0.014044</td>\n",
              "      <td>0.003699</td>\n",
              "      <td>0.009330</td>\n",
              "      <td>0.008967</td>\n",
              "      <td>0.006290</td>\n",
              "      <td>0.007313</td>\n",
              "      <td>0.013566</td>\n",
              "      <td>0.001511</td>\n",
              "      <td>0.002833</td>\n",
              "      <td>0.005504</td>\n",
              "      <td>0.003456</td>\n",
              "      <td>0.004210</td>\n",
              "      <td>0.003239</td>\n",
              "      <td>0.005267</td>\n",
              "      <td>0.001098</td>\n",
              "      <td>0.002910</td>\n",
              "      <td>0.003914</td>\n",
              "      <td>0.000742</td>\n",
              "      <td>0.002519</td>\n",
              "      <td>0.013751</td>\n",
              "      <td>0.008552</td>\n",
              "      <td>0.003736</td>\n",
              "      <td>0.014924</td>\n",
              "      <td>0.006757</td>\n",
              "      <td>0.012755</td>\n",
              "      <td>0.009960</td>\n",
              "      <td>0.017484</td>\n",
              "      <td>0.002665</td>\n",
              "      <td>0.004498</td>\n",
              "      <td>...</td>\n",
              "      <td>0.011870</td>\n",
              "      <td>0.007473</td>\n",
              "      <td>0.003513</td>\n",
              "      <td>0.010152</td>\n",
              "      <td>0.009209</td>\n",
              "      <td>0.009930</td>\n",
              "      <td>0.041379</td>\n",
              "      <td>0.029797</td>\n",
              "      <td>0.000618</td>\n",
              "      <td>0.005187</td>\n",
              "      <td>0.013526</td>\n",
              "      <td>0.012225</td>\n",
              "      <td>0.003561</td>\n",
              "      <td>0.016417</td>\n",
              "      <td>0.018527</td>\n",
              "      <td>0.017725</td>\n",
              "      <td>0.025343</td>\n",
              "      <td>0.030699</td>\n",
              "      <td>0.003375</td>\n",
              "      <td>0.007993</td>\n",
              "      <td>0.002014</td>\n",
              "      <td>0.001469</td>\n",
              "      <td>0.000773</td>\n",
              "      <td>0.002228</td>\n",
              "      <td>0.004599</td>\n",
              "      <td>0.010936</td>\n",
              "      <td>0.000596</td>\n",
              "      <td>0.004077</td>\n",
              "      <td>0.012252</td>\n",
              "      <td>0.009988</td>\n",
              "      <td>0.004611</td>\n",
              "      <td>0.002473</td>\n",
              "      <td>0.002636</td>\n",
              "      <td>0.004195</td>\n",
              "      <td>0.007255</td>\n",
              "      <td>0.009487</td>\n",
              "      <td>0.004316</td>\n",
              "      <td>0.008729</td>\n",
              "      <td>0.010296</td>\n",
              "      <td>0.016437</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.029333</td>\n",
              "      <td>0.006525</td>\n",
              "      <td>0.005727</td>\n",
              "      <td>0.013098</td>\n",
              "      <td>0.007692</td>\n",
              "      <td>0.016358</td>\n",
              "      <td>0.011000</td>\n",
              "      <td>0.015677</td>\n",
              "      <td>0.002344</td>\n",
              "      <td>0.005527</td>\n",
              "      <td>0.006860</td>\n",
              "      <td>0.013586</td>\n",
              "      <td>0.003710</td>\n",
              "      <td>0.009243</td>\n",
              "      <td>0.008994</td>\n",
              "      <td>0.006580</td>\n",
              "      <td>0.007113</td>\n",
              "      <td>0.014127</td>\n",
              "      <td>0.001536</td>\n",
              "      <td>0.002922</td>\n",
              "      <td>0.005472</td>\n",
              "      <td>0.003366</td>\n",
              "      <td>0.004166</td>\n",
              "      <td>0.003233</td>\n",
              "      <td>0.005255</td>\n",
              "      <td>0.001152</td>\n",
              "      <td>0.002889</td>\n",
              "      <td>0.003851</td>\n",
              "      <td>0.000735</td>\n",
              "      <td>0.002525</td>\n",
              "      <td>0.013695</td>\n",
              "      <td>0.008369</td>\n",
              "      <td>0.003819</td>\n",
              "      <td>0.015103</td>\n",
              "      <td>0.006924</td>\n",
              "      <td>0.012921</td>\n",
              "      <td>0.009824</td>\n",
              "      <td>0.017778</td>\n",
              "      <td>0.002605</td>\n",
              "      <td>0.004648</td>\n",
              "      <td>...</td>\n",
              "      <td>0.011968</td>\n",
              "      <td>0.007428</td>\n",
              "      <td>0.003594</td>\n",
              "      <td>0.010037</td>\n",
              "      <td>0.009058</td>\n",
              "      <td>0.009952</td>\n",
              "      <td>0.040614</td>\n",
              "      <td>0.030371</td>\n",
              "      <td>0.000633</td>\n",
              "      <td>0.005188</td>\n",
              "      <td>0.013355</td>\n",
              "      <td>0.011994</td>\n",
              "      <td>0.003674</td>\n",
              "      <td>0.016204</td>\n",
              "      <td>0.018132</td>\n",
              "      <td>0.017880</td>\n",
              "      <td>0.025106</td>\n",
              "      <td>0.030883</td>\n",
              "      <td>0.003316</td>\n",
              "      <td>0.008219</td>\n",
              "      <td>0.002130</td>\n",
              "      <td>0.001476</td>\n",
              "      <td>0.000823</td>\n",
              "      <td>0.002186</td>\n",
              "      <td>0.004413</td>\n",
              "      <td>0.010712</td>\n",
              "      <td>0.000562</td>\n",
              "      <td>0.004160</td>\n",
              "      <td>0.011789</td>\n",
              "      <td>0.009981</td>\n",
              "      <td>0.004694</td>\n",
              "      <td>0.002515</td>\n",
              "      <td>0.002677</td>\n",
              "      <td>0.004222</td>\n",
              "      <td>0.007269</td>\n",
              "      <td>0.009921</td>\n",
              "      <td>0.004387</td>\n",
              "      <td>0.008923</td>\n",
              "      <td>0.010381</td>\n",
              "      <td>0.016914</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 100 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         0         1         2   ...        97        98        99\n",
              "0  0.017943  0.005415  0.003590  ...  0.011075  0.005073  0.012708\n",
              "1  0.021283  0.005215  0.003530  ...  0.009618  0.005946  0.013709\n",
              "2  0.028988  0.006511  0.005591  ...  0.008791  0.010040  0.016301\n",
              "3  0.029534  0.006471  0.005615  ...  0.008729  0.010296  0.016437\n",
              "4  0.029333  0.006525  0.005727  ...  0.008923  0.010381  0.016914\n",
              "\n",
              "[5 rows x 100 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYIKxV9BkzhS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#convert to an array and scale the data\n",
        "X=np.array(TNet);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHuKaNkHkzhU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X=MinMaxScaler(feature_range=(0, 1), copy=True).fit_transform(X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gu4LSECmkzhW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2dc532b9-dbba-437e-8395-d854f882928d"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(669, 100)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ErlgDYPBkzhY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "377749e1-dca8-46c8-e7d2-6611df657a8a"
      },
      "source": [
        "#define day of the week corresponding to each day of observation; 0-Sunday, 1-Monday,...,6-Saturday\n",
        "y=np.array(range(669))%7; y[:10]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 2, 3, 4, 5, 6, 0, 1, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayvjP2wjkzha",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "ed9ee0c4-ac15-42d7-8e27-cb9a68e76d6a"
      },
      "source": [
        "yc=np_utils.to_categorical(y) #get categorical binary variables isSunday, isMonday,...\n",
        "yc[:5]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 1., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 1., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSB4kHhYkzhc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_test=X[400:,:]; X_train=X[:400,:]; #split the data into training and test\n",
        "y_test=yc[400:,:]; y_train=yc[:400,:]\n",
        "dim = X_train.shape[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8qkzBH1qqoW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "2f936df1-a179-4ee8-b763-614fb3ed5cee"
      },
      "source": [
        "print(X_test.shape)\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(269, 100)\n",
            "(400, 100)\n",
            "(400, 7)\n",
            "(269, 7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoVuc7aOkzhi",
        "colab_type": "text"
      },
      "source": [
        "## Task 1. Classify weekdays/weekends\n",
        "Label the rows with ones for weekends, zeros for weekdays.\n",
        "Train a neural network with 4 layers of 30,10,3 and 1 (output) neurons over the training sample against this label, evaluating its performance over the test sample. Report the acheived accuracy (categorical) over the test sample\n",
        "\n",
        "First three layers use relu activation function, last one - sigmoid.\n",
        "Use loss='binary_crossentropy', optimizer='adam', 100 epochs, batch_size=20. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_rdDQtRnbDy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "outputId": "d1f4ae33-01b4-4ea6-f942-e9d0c715a6d6"
      },
      "source": [
        "y"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0,\n",
              "       1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1,\n",
              "       2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2,\n",
              "       3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3,\n",
              "       4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4,\n",
              "       5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5,\n",
              "       6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6,\n",
              "       0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0,\n",
              "       1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1,\n",
              "       2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2,\n",
              "       3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3,\n",
              "       4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4,\n",
              "       5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5,\n",
              "       6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6,\n",
              "       0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0,\n",
              "       1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1,\n",
              "       2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2,\n",
              "       3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3,\n",
              "       4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4,\n",
              "       5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5,\n",
              "       6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6,\n",
              "       0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0,\n",
              "       1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1,\n",
              "       2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2,\n",
              "       3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3,\n",
              "       4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4,\n",
              "       5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5,\n",
              "       6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6,\n",
              "       0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0,\n",
              "       1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1,\n",
              "       2, 3, 4, 5, 6, 0, 1, 2, 3])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIzdBDqsk-9k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "outputId": "02c2ae66-602c-4186-dc9b-901b2c86718a"
      },
      "source": [
        "y_c = y\n",
        "for idx, item in enumerate(y_c):\n",
        "  if ((item == 0)| (item == 6)):\n",
        "    y_c[idx] = 1\n",
        "  else:\n",
        "    y_c[idx] = 0\n",
        "\n",
        "y_c"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,\n",
              "       0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,\n",
              "       0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
              "       0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
              "       0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
              "       0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
              "       1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n",
              "       1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,\n",
              "       0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,\n",
              "       0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
              "       0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
              "       0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
              "       0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
              "       1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n",
              "       1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,\n",
              "       0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,\n",
              "       0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
              "       0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
              "       0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
              "       0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
              "       1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n",
              "       1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,\n",
              "       0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,\n",
              "       0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
              "       0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
              "       0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
              "       0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
              "       1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n",
              "       1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,\n",
              "       0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,\n",
              "       0, 0, 0, 0, 1, 1, 0, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0W7JKnzHrZzF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y1_train=y_c[:400]\n",
        "y1_test=y_c[400:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4F6T__gq0C9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "83510a02-4091-4e98-f293-c446be5ff3b3"
      },
      "source": [
        "print(y1_train.shape)\n",
        "print(y1_test.shape)\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(400,)\n",
            "(269,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNPlpyQQnwTy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "32615429-d97f-4533-dc77-2cdf9c3a0021"
      },
      "source": [
        "np.random.seed(1612)\n",
        "test = pd.DataFrame()\n",
        "model = Sequential()\n",
        "model.add(Dense(30, activation='relu', input_dim=dim))\n",
        "model.add(Dense(10, activation='relu'))\n",
        "model.add(Dense(3, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(X_train, y1_train, validation_data=(X_test, y1_test), epochs=100, batch_size=20, verbose=2)\n",
        "test = model.predict(X_test)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 400 samples, validate on 269 samples\n",
            "Epoch 1/100\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            " - 0s - loss: 0.4330 - acc: 0.7125 - val_loss: 0.3422 - val_acc: 0.7175\n",
            "Epoch 2/100\n",
            " - 0s - loss: 0.3220 - acc: 0.7125 - val_loss: 0.2880 - val_acc: 0.7175\n",
            "Epoch 3/100\n",
            " - 0s - loss: 0.2805 - acc: 0.7350 - val_loss: 0.2581 - val_acc: 0.7955\n",
            "Epoch 4/100\n",
            " - 0s - loss: 0.2407 - acc: 0.9300 - val_loss: 0.2128 - val_acc: 0.9554\n",
            "Epoch 5/100\n",
            " - 0s - loss: 0.2041 - acc: 0.9525 - val_loss: 0.1830 - val_acc: 0.9554\n",
            "Epoch 6/100\n",
            " - 0s - loss: 0.1761 - acc: 0.9525 - val_loss: 0.1605 - val_acc: 0.9554\n",
            "Epoch 7/100\n",
            " - 0s - loss: 0.1547 - acc: 0.9525 - val_loss: 0.1456 - val_acc: 0.9554\n",
            "Epoch 8/100\n",
            " - 0s - loss: 0.1423 - acc: 0.9525 - val_loss: 0.1371 - val_acc: 0.9554\n",
            "Epoch 9/100\n",
            " - 0s - loss: 0.1370 - acc: 0.9525 - val_loss: 0.1313 - val_acc: 0.9554\n",
            "Epoch 10/100\n",
            " - 0s - loss: 0.1319 - acc: 0.9525 - val_loss: 0.1279 - val_acc: 0.9554\n",
            "Epoch 11/100\n",
            " - 0s - loss: 0.1338 - acc: 0.9550 - val_loss: 0.1282 - val_acc: 0.9517\n",
            "Epoch 12/100\n",
            " - 0s - loss: 0.1266 - acc: 0.9525 - val_loss: 0.1240 - val_acc: 0.9554\n",
            "Epoch 13/100\n",
            " - 0s - loss: 0.1225 - acc: 0.9525 - val_loss: 0.1224 - val_acc: 0.9554\n",
            "Epoch 14/100\n",
            " - 0s - loss: 0.1190 - acc: 0.9525 - val_loss: 0.1199 - val_acc: 0.9554\n",
            "Epoch 15/100\n",
            " - 0s - loss: 0.1179 - acc: 0.9525 - val_loss: 0.1189 - val_acc: 0.9554\n",
            "Epoch 16/100\n",
            " - 0s - loss: 0.1212 - acc: 0.9550 - val_loss: 0.1166 - val_acc: 0.9554\n",
            "Epoch 17/100\n",
            " - 0s - loss: 0.1152 - acc: 0.9550 - val_loss: 0.1163 - val_acc: 0.9554\n",
            "Epoch 18/100\n",
            " - 0s - loss: 0.1080 - acc: 0.9550 - val_loss: 0.1130 - val_acc: 0.9554\n",
            "Epoch 19/100\n",
            " - 0s - loss: 0.1088 - acc: 0.9525 - val_loss: 0.1111 - val_acc: 0.9554\n",
            "Epoch 20/100\n",
            " - 0s - loss: 0.1062 - acc: 0.9550 - val_loss: 0.1094 - val_acc: 0.9554\n",
            "Epoch 21/100\n",
            " - 0s - loss: 0.1048 - acc: 0.9550 - val_loss: 0.1072 - val_acc: 0.9554\n",
            "Epoch 22/100\n",
            " - 0s - loss: 0.0979 - acc: 0.9575 - val_loss: 0.1058 - val_acc: 0.9554\n",
            "Epoch 23/100\n",
            " - 0s - loss: 0.0961 - acc: 0.9575 - val_loss: 0.1055 - val_acc: 0.9554\n",
            "Epoch 24/100\n",
            " - 0s - loss: 0.0915 - acc: 0.9550 - val_loss: 0.1016 - val_acc: 0.9628\n",
            "Epoch 25/100\n",
            " - 0s - loss: 0.0894 - acc: 0.9575 - val_loss: 0.1014 - val_acc: 0.9591\n",
            "Epoch 26/100\n",
            " - 0s - loss: 0.0859 - acc: 0.9600 - val_loss: 0.0982 - val_acc: 0.9628\n",
            "Epoch 27/100\n",
            " - 0s - loss: 0.0833 - acc: 0.9600 - val_loss: 0.0981 - val_acc: 0.9628\n",
            "Epoch 28/100\n",
            " - 0s - loss: 0.0820 - acc: 0.9575 - val_loss: 0.0959 - val_acc: 0.9628\n",
            "Epoch 29/100\n",
            " - 0s - loss: 0.0812 - acc: 0.9600 - val_loss: 0.0946 - val_acc: 0.9628\n",
            "Epoch 30/100\n",
            " - 0s - loss: 0.0773 - acc: 0.9600 - val_loss: 0.0942 - val_acc: 0.9628\n",
            "Epoch 31/100\n",
            " - 0s - loss: 0.0738 - acc: 0.9650 - val_loss: 0.0910 - val_acc: 0.9628\n",
            "Epoch 32/100\n",
            " - 0s - loss: 0.0718 - acc: 0.9650 - val_loss: 0.0907 - val_acc: 0.9628\n",
            "Epoch 33/100\n",
            " - 0s - loss: 0.0709 - acc: 0.9625 - val_loss: 0.0893 - val_acc: 0.9628\n",
            "Epoch 34/100\n",
            " - 0s - loss: 0.0682 - acc: 0.9650 - val_loss: 0.0887 - val_acc: 0.9628\n",
            "Epoch 35/100\n",
            " - 0s - loss: 0.0669 - acc: 0.9650 - val_loss: 0.0882 - val_acc: 0.9628\n",
            "Epoch 36/100\n",
            " - 0s - loss: 0.0663 - acc: 0.9675 - val_loss: 0.0884 - val_acc: 0.9591\n",
            "Epoch 37/100\n",
            " - 0s - loss: 0.0665 - acc: 0.9650 - val_loss: 0.0865 - val_acc: 0.9628\n",
            "Epoch 38/100\n",
            " - 0s - loss: 0.0620 - acc: 0.9675 - val_loss: 0.0840 - val_acc: 0.9665\n",
            "Epoch 39/100\n",
            " - 0s - loss: 0.0615 - acc: 0.9675 - val_loss: 0.0877 - val_acc: 0.9628\n",
            "Epoch 40/100\n",
            " - 0s - loss: 0.0580 - acc: 0.9775 - val_loss: 0.0822 - val_acc: 0.9665\n",
            "Epoch 41/100\n",
            " - 0s - loss: 0.0538 - acc: 0.9800 - val_loss: 0.0827 - val_acc: 0.9740\n",
            "Epoch 42/100\n",
            " - 0s - loss: 0.0475 - acc: 0.9825 - val_loss: 0.0857 - val_acc: 0.9665\n",
            "Epoch 43/100\n",
            " - 0s - loss: 0.0488 - acc: 0.9825 - val_loss: 0.0855 - val_acc: 0.9665\n",
            "Epoch 44/100\n",
            " - 0s - loss: 0.0451 - acc: 0.9800 - val_loss: 0.0863 - val_acc: 0.9740\n",
            "Epoch 45/100\n",
            " - 0s - loss: 0.0443 - acc: 0.9875 - val_loss: 0.0849 - val_acc: 0.9703\n",
            "Epoch 46/100\n",
            " - 0s - loss: 0.0402 - acc: 0.9900 - val_loss: 0.0860 - val_acc: 0.9665\n",
            "Epoch 47/100\n",
            " - 0s - loss: 0.0421 - acc: 0.9875 - val_loss: 0.0844 - val_acc: 0.9703\n",
            "Epoch 48/100\n",
            " - 0s - loss: 0.0383 - acc: 0.9875 - val_loss: 0.0850 - val_acc: 0.9665\n",
            "Epoch 49/100\n",
            " - 0s - loss: 0.0365 - acc: 0.9875 - val_loss: 0.0850 - val_acc: 0.9740\n",
            "Epoch 50/100\n",
            " - 0s - loss: 0.0358 - acc: 0.9875 - val_loss: 0.0898 - val_acc: 0.9591\n",
            "Epoch 51/100\n",
            " - 0s - loss: 0.0343 - acc: 0.9925 - val_loss: 0.0868 - val_acc: 0.9740\n",
            "Epoch 52/100\n",
            " - 0s - loss: 0.0359 - acc: 0.9875 - val_loss: 0.0851 - val_acc: 0.9665\n",
            "Epoch 53/100\n",
            " - 0s - loss: 0.0318 - acc: 0.9950 - val_loss: 0.0848 - val_acc: 0.9665\n",
            "Epoch 54/100\n",
            " - 0s - loss: 0.0318 - acc: 0.9925 - val_loss: 0.0896 - val_acc: 0.9740\n",
            "Epoch 55/100\n",
            " - 0s - loss: 0.0281 - acc: 0.9925 - val_loss: 0.0841 - val_acc: 0.9665\n",
            "Epoch 56/100\n",
            " - 0s - loss: 0.0311 - acc: 0.9925 - val_loss: 0.0829 - val_acc: 0.9703\n",
            "Epoch 57/100\n",
            " - 0s - loss: 0.0275 - acc: 0.9975 - val_loss: 0.0826 - val_acc: 0.9703\n",
            "Epoch 58/100\n",
            " - 0s - loss: 0.0300 - acc: 0.9900 - val_loss: 0.0866 - val_acc: 0.9740\n",
            "Epoch 59/100\n",
            " - 0s - loss: 0.0266 - acc: 0.9900 - val_loss: 0.0839 - val_acc: 0.9703\n",
            "Epoch 60/100\n",
            " - 0s - loss: 0.0285 - acc: 0.9950 - val_loss: 0.0847 - val_acc: 0.9740\n",
            "Epoch 61/100\n",
            " - 0s - loss: 0.0233 - acc: 0.9950 - val_loss: 0.0829 - val_acc: 0.9703\n",
            "Epoch 62/100\n",
            " - 0s - loss: 0.0250 - acc: 0.9950 - val_loss: 0.0834 - val_acc: 0.9740\n",
            "Epoch 63/100\n",
            " - 0s - loss: 0.0244 - acc: 0.9950 - val_loss: 0.0817 - val_acc: 0.9740\n",
            "Epoch 64/100\n",
            " - 0s - loss: 0.0241 - acc: 0.9925 - val_loss: 0.0778 - val_acc: 0.9665\n",
            "Epoch 65/100\n",
            " - 0s - loss: 0.0229 - acc: 0.9950 - val_loss: 0.0808 - val_acc: 0.9740\n",
            "Epoch 66/100\n",
            " - 0s - loss: 0.0215 - acc: 0.9950 - val_loss: 0.0774 - val_acc: 0.9703\n",
            "Epoch 67/100\n",
            " - 0s - loss: 0.0217 - acc: 0.9975 - val_loss: 0.0878 - val_acc: 0.9740\n",
            "Epoch 68/100\n",
            " - 0s - loss: 0.0200 - acc: 0.9950 - val_loss: 0.0798 - val_acc: 0.9740\n",
            "Epoch 69/100\n",
            " - 0s - loss: 0.0188 - acc: 0.9975 - val_loss: 0.0777 - val_acc: 0.9740\n",
            "Epoch 70/100\n",
            " - 0s - loss: 0.0186 - acc: 0.9975 - val_loss: 0.0759 - val_acc: 0.9740\n",
            "Epoch 71/100\n",
            " - 0s - loss: 0.0176 - acc: 0.9950 - val_loss: 0.0776 - val_acc: 0.9740\n",
            "Epoch 72/100\n",
            " - 0s - loss: 0.0172 - acc: 0.9975 - val_loss: 0.0802 - val_acc: 0.9740\n",
            "Epoch 73/100\n",
            " - 0s - loss: 0.0158 - acc: 0.9950 - val_loss: 0.0740 - val_acc: 0.9740\n",
            "Epoch 74/100\n",
            " - 0s - loss: 0.0219 - acc: 0.9925 - val_loss: 0.0769 - val_acc: 0.9740\n",
            "Epoch 75/100\n",
            " - 0s - loss: 0.0195 - acc: 0.9900 - val_loss: 0.0769 - val_acc: 0.9740\n",
            "Epoch 76/100\n",
            " - 0s - loss: 0.0158 - acc: 0.9950 - val_loss: 0.0764 - val_acc: 0.9740\n",
            "Epoch 77/100\n",
            " - 0s - loss: 0.0208 - acc: 0.9900 - val_loss: 0.0948 - val_acc: 0.9740\n",
            "Epoch 78/100\n",
            " - 0s - loss: 0.0162 - acc: 0.9925 - val_loss: 0.0709 - val_acc: 0.9740\n",
            "Epoch 79/100\n",
            " - 0s - loss: 0.0164 - acc: 0.9950 - val_loss: 0.0713 - val_acc: 0.9740\n",
            "Epoch 80/100\n",
            " - 0s - loss: 0.0159 - acc: 0.9925 - val_loss: 0.0719 - val_acc: 0.9740\n",
            "Epoch 81/100\n",
            " - 0s - loss: 0.0125 - acc: 0.9975 - val_loss: 0.0834 - val_acc: 0.9740\n",
            "Epoch 82/100\n",
            " - 0s - loss: 0.0172 - acc: 0.9950 - val_loss: 0.0689 - val_acc: 0.9740\n",
            "Epoch 83/100\n",
            " - 0s - loss: 0.0135 - acc: 0.9975 - val_loss: 0.0723 - val_acc: 0.9740\n",
            "Epoch 84/100\n",
            " - 0s - loss: 0.0119 - acc: 1.0000 - val_loss: 0.0803 - val_acc: 0.9740\n",
            "Epoch 85/100\n",
            " - 0s - loss: 0.0121 - acc: 0.9925 - val_loss: 0.0692 - val_acc: 0.9777\n",
            "Epoch 86/100\n",
            " - 0s - loss: 0.0134 - acc: 0.9975 - val_loss: 0.0726 - val_acc: 0.9740\n",
            "Epoch 87/100\n",
            " - 0s - loss: 0.0101 - acc: 0.9975 - val_loss: 0.0733 - val_acc: 0.9740\n",
            "Epoch 88/100\n",
            " - 0s - loss: 0.0100 - acc: 0.9975 - val_loss: 0.0691 - val_acc: 0.9777\n",
            "Epoch 89/100\n",
            " - 0s - loss: 0.0123 - acc: 0.9950 - val_loss: 0.0809 - val_acc: 0.9740\n",
            "Epoch 90/100\n",
            " - 0s - loss: 0.0115 - acc: 0.9975 - val_loss: 0.0742 - val_acc: 0.9740\n",
            "Epoch 91/100\n",
            " - 0s - loss: 0.0091 - acc: 0.9975 - val_loss: 0.0688 - val_acc: 0.9777\n",
            "Epoch 92/100\n",
            " - 0s - loss: 0.0128 - acc: 0.9975 - val_loss: 0.0897 - val_acc: 0.9740\n",
            "Epoch 93/100\n",
            " - 0s - loss: 0.0105 - acc: 0.9975 - val_loss: 0.0746 - val_acc: 0.9740\n",
            "Epoch 94/100\n",
            " - 0s - loss: 0.0086 - acc: 0.9975 - val_loss: 0.0672 - val_acc: 0.9777\n",
            "Epoch 95/100\n",
            " - 0s - loss: 0.0095 - acc: 0.9950 - val_loss: 0.0680 - val_acc: 0.9777\n",
            "Epoch 96/100\n",
            " - 0s - loss: 0.0093 - acc: 0.9975 - val_loss: 0.0755 - val_acc: 0.9740\n",
            "Epoch 97/100\n",
            " - 0s - loss: 0.0078 - acc: 1.0000 - val_loss: 0.0684 - val_acc: 0.9740\n",
            "Epoch 98/100\n",
            " - 0s - loss: 0.0070 - acc: 1.0000 - val_loss: 0.0743 - val_acc: 0.9740\n",
            "Epoch 99/100\n",
            " - 0s - loss: 0.0072 - acc: 1.0000 - val_loss: 0.0694 - val_acc: 0.9740\n",
            "Epoch 100/100\n",
            " - 0s - loss: 0.0072 - acc: 1.0000 - val_loss: 0.0695 - val_acc: 0.9740\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AntQNMTcsitc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "8e180242-11a9-4e47-c75b-c0bfa0d9bc55"
      },
      "source": [
        "acc_df = pd.DataFrame()\n",
        "acc_df['predict'] = test.flatten()\n",
        "acc_df.head()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>predict</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.000004</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000009</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.000002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.000001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.000044</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    predict\n",
              "0  0.000004\n",
              "1  0.000009\n",
              "2  0.000002\n",
              "3  0.000001\n",
              "4  0.000044"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDUeCAD2tu3k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "98f02702-6252-43e0-b87a-6fbcfe1b0d40"
      },
      "source": [
        "acc_df['labels'] = y1_test\n",
        "acc_df.head(10)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>predict</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.000004</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000009</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.000002</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.000001</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.000044</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.999995</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.999994</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.000007</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.000015</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.000657</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    predict  labels\n",
              "0  0.000004       0\n",
              "1  0.000009       0\n",
              "2  0.000002       0\n",
              "3  0.000001       0\n",
              "4  0.000044       0\n",
              "5  0.999995       1\n",
              "6  0.999994       1\n",
              "7  0.000007       0\n",
              "8  0.000015       0\n",
              "9  0.000657       0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSOEezEQt88e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "b0002cca-9644-460a-e68a-a9436f28f10c"
      },
      "source": [
        "print(\"Accuracy is: {}\\nBUDAM BUM!!\".format(1.0*sum((acc_df['labels']==1)==(acc_df['predict']>0.5))/len(acc_df['predict'])))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy is: 0.9739776951672863\n",
            "BUDAM BUM!!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnmCUpqxkzhj",
        "colab_type": "text"
      },
      "source": [
        "## Task 2. Classify all days of the week\n",
        "Train a neural network against the origial categorical label. Use 5 layers of 40,15,5 and 7 (outputs, representing probabilities for a current input to correspond to each of the weekdays) neurons over the training sample, evaluating its performance over the test sample (use 'categorical_accurary'). Report the acheived accuracy (categorical) over the test sample.\n",
        "\n",
        "First three layers use relu activation function, last one - sigmoid.\n",
        "Use loss='binary_crossentropy', optimizer='adam', 200 epochs, batch_size=20"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhIEx6zhwaM6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "07f4950d-754a-4bca-e0e5-1da90999dff2"
      },
      "source": [
        "y=np.array(range(669))%7; y[:16]"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvVuoqTIxhYR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = list(y)\n",
        "for i, item in  enumerate(y):\n",
        "  size = y.count(item)\n",
        "  y[i] = item/size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfifqB4c0-zw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y2_train=y[:400]\n",
        "y2_test=y[400:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gT4CTUAVkzhj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "22a68220-2674-4606-f047-3af996e9628e"
      },
      "source": [
        "np.random.seed(161219)\n",
        "model = Sequential()\n",
        "model.add(Dense(40, activation='relu', input_dim=dim))\n",
        "model.add(Dense(15, activation='relu'))\n",
        "model.add(Dense(5, activation='relu'))\n",
        "model.add(Dense(7, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(X_train, y2_train, validation_data=(X_test, y2_test), epochs=200, batch_size=20, verbose=2)\n",
        "test2 = model.predict(X_test)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 400 samples, validate on 269 samples\n",
            "Epoch 1/200\n",
            " - 0s - loss: 0.5991 - acc: 0.1450 - val_loss: 0.6297 - val_acc: 0.1413\n",
            "Epoch 2/200\n",
            " - 0s - loss: 0.3747 - acc: 0.1450 - val_loss: 0.7069 - val_acc: 0.1413\n",
            "Epoch 3/200\n",
            " - 0s - loss: 0.2094 - acc: 0.1450 - val_loss: 1.0466 - val_acc: 0.1413\n",
            "Epoch 4/200\n",
            " - 0s - loss: 0.1980 - acc: 0.1450 - val_loss: 1.0311 - val_acc: 0.1413\n",
            "Epoch 5/200\n",
            " - 0s - loss: 0.1957 - acc: 0.1450 - val_loss: 0.9698 - val_acc: 0.1413\n",
            "Epoch 6/200\n",
            " - 0s - loss: 0.1948 - acc: 0.1450 - val_loss: 0.9866 - val_acc: 0.1413\n",
            "Epoch 7/200\n",
            " - 0s - loss: 0.1940 - acc: 0.1450 - val_loss: 0.9789 - val_acc: 0.1413\n",
            "Epoch 8/200\n",
            " - 0s - loss: 0.1932 - acc: 0.1450 - val_loss: 0.9746 - val_acc: 0.1413\n",
            "Epoch 9/200\n",
            " - 0s - loss: 0.1925 - acc: 0.1450 - val_loss: 0.9792 - val_acc: 0.1413\n",
            "Epoch 10/200\n",
            " - 0s - loss: 0.1918 - acc: 0.1450 - val_loss: 0.9767 - val_acc: 0.1413\n",
            "Epoch 11/200\n",
            " - 0s - loss: 0.1913 - acc: 0.1450 - val_loss: 0.9696 - val_acc: 0.1413\n",
            "Epoch 12/200\n",
            " - 0s - loss: 0.1909 - acc: 0.1450 - val_loss: 0.9821 - val_acc: 0.1413\n",
            "Epoch 13/200\n",
            " - 0s - loss: 0.1905 - acc: 0.1450 - val_loss: 0.9737 - val_acc: 0.1413\n",
            "Epoch 14/200\n",
            " - 0s - loss: 0.1901 - acc: 0.1450 - val_loss: 0.9768 - val_acc: 0.1413\n",
            "Epoch 15/200\n",
            " - 0s - loss: 0.1897 - acc: 0.1450 - val_loss: 0.9612 - val_acc: 0.1413\n",
            "Epoch 16/200\n",
            " - 0s - loss: 0.1891 - acc: 0.1450 - val_loss: 0.9654 - val_acc: 0.1413\n",
            "Epoch 17/200\n",
            " - 0s - loss: 0.1888 - acc: 0.1450 - val_loss: 0.9567 - val_acc: 0.1413\n",
            "Epoch 18/200\n",
            " - 0s - loss: 0.1882 - acc: 0.1450 - val_loss: 0.9120 - val_acc: 0.1413\n",
            "Epoch 19/200\n",
            " - 0s - loss: 0.1879 - acc: 0.1450 - val_loss: 0.9651 - val_acc: 0.1413\n",
            "Epoch 20/200\n",
            " - 0s - loss: 0.1873 - acc: 0.1450 - val_loss: 0.9646 - val_acc: 0.1413\n",
            "Epoch 21/200\n",
            " - 0s - loss: 0.1870 - acc: 0.1450 - val_loss: 0.9932 - val_acc: 0.1413\n",
            "Epoch 22/200\n",
            " - 0s - loss: 0.1867 - acc: 0.1450 - val_loss: 0.9936 - val_acc: 0.1413\n",
            "Epoch 23/200\n",
            " - 0s - loss: 0.1862 - acc: 0.1450 - val_loss: 0.9721 - val_acc: 0.1413\n",
            "Epoch 24/200\n",
            " - 0s - loss: 0.1859 - acc: 0.1450 - val_loss: 0.9786 - val_acc: 0.1413\n",
            "Epoch 25/200\n",
            " - 0s - loss: 0.1856 - acc: 0.1450 - val_loss: 0.9626 - val_acc: 0.1413\n",
            "Epoch 26/200\n",
            " - 0s - loss: 0.1851 - acc: 0.1450 - val_loss: 0.9451 - val_acc: 0.1413\n",
            "Epoch 27/200\n",
            " - 0s - loss: 0.1849 - acc: 0.1450 - val_loss: 0.9610 - val_acc: 0.1413\n",
            "Epoch 28/200\n",
            " - 0s - loss: 0.1847 - acc: 0.1450 - val_loss: 0.9330 - val_acc: 0.1413\n",
            "Epoch 29/200\n",
            " - 0s - loss: 0.1844 - acc: 0.1450 - val_loss: 0.9425 - val_acc: 0.1413\n",
            "Epoch 30/200\n",
            " - 0s - loss: 0.1841 - acc: 0.1450 - val_loss: 0.9529 - val_acc: 0.1413\n",
            "Epoch 31/200\n",
            " - 0s - loss: 0.1839 - acc: 0.1450 - val_loss: 0.9210 - val_acc: 0.1413\n",
            "Epoch 32/200\n",
            " - 0s - loss: 0.1841 - acc: 0.1450 - val_loss: 0.9575 - val_acc: 0.1413\n",
            "Epoch 33/200\n",
            " - 0s - loss: 0.1837 - acc: 0.1450 - val_loss: 0.9402 - val_acc: 0.1413\n",
            "Epoch 34/200\n",
            " - 0s - loss: 0.1835 - acc: 0.1450 - val_loss: 0.9185 - val_acc: 0.1413\n",
            "Epoch 35/200\n",
            " - 0s - loss: 0.1837 - acc: 0.1450 - val_loss: 1.0000 - val_acc: 0.1413\n",
            "Epoch 36/200\n",
            " - 0s - loss: 0.1834 - acc: 0.1450 - val_loss: 0.9299 - val_acc: 0.1413\n",
            "Epoch 37/200\n",
            " - 0s - loss: 0.1831 - acc: 0.1450 - val_loss: 0.9203 - val_acc: 0.1413\n",
            "Epoch 38/200\n",
            " - 0s - loss: 0.1830 - acc: 0.1450 - val_loss: 0.9317 - val_acc: 0.1413\n",
            "Epoch 39/200\n",
            " - 0s - loss: 0.1827 - acc: 0.1450 - val_loss: 0.9346 - val_acc: 0.1413\n",
            "Epoch 40/200\n",
            " - 0s - loss: 0.1826 - acc: 0.1450 - val_loss: 0.9273 - val_acc: 0.1413\n",
            "Epoch 41/200\n",
            " - 0s - loss: 0.1828 - acc: 0.1450 - val_loss: 0.9477 - val_acc: 0.1413\n",
            "Epoch 42/200\n",
            " - 0s - loss: 0.1824 - acc: 0.1450 - val_loss: 0.9357 - val_acc: 0.1413\n",
            "Epoch 43/200\n",
            " - 0s - loss: 0.1821 - acc: 0.1450 - val_loss: 0.9918 - val_acc: 0.1413\n",
            "Epoch 44/200\n",
            " - 0s - loss: 0.1827 - acc: 0.1450 - val_loss: 0.8430 - val_acc: 0.1413\n",
            "Epoch 45/200\n",
            " - 0s - loss: 0.1829 - acc: 0.1450 - val_loss: 0.9532 - val_acc: 0.1413\n",
            "Epoch 46/200\n",
            " - 0s - loss: 0.1821 - acc: 0.1450 - val_loss: 0.9506 - val_acc: 0.1413\n",
            "Epoch 47/200\n",
            " - 0s - loss: 0.1819 - acc: 0.1450 - val_loss: 0.9476 - val_acc: 0.1413\n",
            "Epoch 48/200\n",
            " - 0s - loss: 0.1819 - acc: 0.1450 - val_loss: 0.9240 - val_acc: 0.1413\n",
            "Epoch 49/200\n",
            " - 0s - loss: 0.1817 - acc: 0.1450 - val_loss: 0.8803 - val_acc: 0.1413\n",
            "Epoch 50/200\n",
            " - 0s - loss: 0.1816 - acc: 0.1450 - val_loss: 0.9332 - val_acc: 0.1413\n",
            "Epoch 51/200\n",
            " - 0s - loss: 0.1814 - acc: 0.1450 - val_loss: 0.9449 - val_acc: 0.1413\n",
            "Epoch 52/200\n",
            " - 0s - loss: 0.1814 - acc: 0.1450 - val_loss: 0.9289 - val_acc: 0.1413\n",
            "Epoch 53/200\n",
            " - 0s - loss: 0.1813 - acc: 0.1450 - val_loss: 0.9049 - val_acc: 0.1413\n",
            "Epoch 54/200\n",
            " - 0s - loss: 0.1816 - acc: 0.1450 - val_loss: 0.9526 - val_acc: 0.1413\n",
            "Epoch 55/200\n",
            " - 0s - loss: 0.1813 - acc: 0.1450 - val_loss: 0.8962 - val_acc: 0.1413\n",
            "Epoch 56/200\n",
            " - 0s - loss: 0.1815 - acc: 0.1450 - val_loss: 0.9407 - val_acc: 0.1413\n",
            "Epoch 57/200\n",
            " - 0s - loss: 0.1811 - acc: 0.1450 - val_loss: 0.9441 - val_acc: 0.1413\n",
            "Epoch 58/200\n",
            " - 0s - loss: 0.1808 - acc: 0.1450 - val_loss: 0.9059 - val_acc: 0.1413\n",
            "Epoch 59/200\n",
            " - 0s - loss: 0.1811 - acc: 0.1450 - val_loss: 0.9522 - val_acc: 0.1413\n",
            "Epoch 60/200\n",
            " - 0s - loss: 0.1810 - acc: 0.1450 - val_loss: 0.9218 - val_acc: 0.1413\n",
            "Epoch 61/200\n",
            " - 0s - loss: 0.1807 - acc: 0.1450 - val_loss: 0.9257 - val_acc: 0.1413\n",
            "Epoch 62/200\n",
            " - 0s - loss: 0.1807 - acc: 0.1450 - val_loss: 0.9167 - val_acc: 0.1413\n",
            "Epoch 63/200\n",
            " - 0s - loss: 0.1808 - acc: 0.1450 - val_loss: 0.9492 - val_acc: 0.1413\n",
            "Epoch 64/200\n",
            " - 0s - loss: 0.1804 - acc: 0.1450 - val_loss: 0.9049 - val_acc: 0.1413\n",
            "Epoch 65/200\n",
            " - 0s - loss: 0.1805 - acc: 0.1450 - val_loss: 0.9119 - val_acc: 0.1413\n",
            "Epoch 66/200\n",
            " - 0s - loss: 0.1804 - acc: 0.1450 - val_loss: 0.9423 - val_acc: 0.1413\n",
            "Epoch 67/200\n",
            " - 0s - loss: 0.1804 - acc: 0.1450 - val_loss: 0.9177 - val_acc: 0.1413\n",
            "Epoch 68/200\n",
            " - 0s - loss: 0.1803 - acc: 0.1450 - val_loss: 0.9279 - val_acc: 0.1413\n",
            "Epoch 69/200\n",
            " - 0s - loss: 0.1801 - acc: 0.1450 - val_loss: 0.9200 - val_acc: 0.1413\n",
            "Epoch 70/200\n",
            " - 0s - loss: 0.1803 - acc: 0.1450 - val_loss: 0.9653 - val_acc: 0.1413\n",
            "Epoch 71/200\n",
            " - 0s - loss: 0.1803 - acc: 0.1450 - val_loss: 0.8786 - val_acc: 0.1413\n",
            "Epoch 72/200\n",
            " - 0s - loss: 0.1799 - acc: 0.1450 - val_loss: 0.9191 - val_acc: 0.1413\n",
            "Epoch 73/200\n",
            " - 0s - loss: 0.1798 - acc: 0.1450 - val_loss: 0.9258 - val_acc: 0.1413\n",
            "Epoch 74/200\n",
            " - 0s - loss: 0.1800 - acc: 0.1450 - val_loss: 0.8714 - val_acc: 0.1413\n",
            "Epoch 75/200\n",
            " - 0s - loss: 0.1797 - acc: 0.1450 - val_loss: 0.9030 - val_acc: 0.1413\n",
            "Epoch 76/200\n",
            " - 0s - loss: 0.1798 - acc: 0.1450 - val_loss: 0.9011 - val_acc: 0.1413\n",
            "Epoch 77/200\n",
            " - 0s - loss: 0.1801 - acc: 0.1450 - val_loss: 0.9642 - val_acc: 0.1413\n",
            "Epoch 78/200\n",
            " - 0s - loss: 0.1798 - acc: 0.1450 - val_loss: 0.8735 - val_acc: 0.1413\n",
            "Epoch 79/200\n",
            " - 0s - loss: 0.1799 - acc: 0.1450 - val_loss: 0.9584 - val_acc: 0.1413\n",
            "Epoch 80/200\n",
            " - 0s - loss: 0.1799 - acc: 0.1450 - val_loss: 0.8787 - val_acc: 0.1413\n",
            "Epoch 81/200\n",
            " - 0s - loss: 0.1796 - acc: 0.1450 - val_loss: 0.9447 - val_acc: 0.1413\n",
            "Epoch 82/200\n",
            " - 0s - loss: 0.1794 - acc: 0.1450 - val_loss: 0.9102 - val_acc: 0.1413\n",
            "Epoch 83/200\n",
            " - 0s - loss: 0.1793 - acc: 0.1450 - val_loss: 0.8800 - val_acc: 0.1413\n",
            "Epoch 84/200\n",
            " - 0s - loss: 0.1792 - acc: 0.1450 - val_loss: 0.8939 - val_acc: 0.1413\n",
            "Epoch 85/200\n",
            " - 0s - loss: 0.1797 - acc: 0.1450 - val_loss: 0.9582 - val_acc: 0.1413\n",
            "Epoch 86/200\n",
            " - 0s - loss: 0.1794 - acc: 0.1450 - val_loss: 0.8662 - val_acc: 0.1413\n",
            "Epoch 87/200\n",
            " - 0s - loss: 0.1793 - acc: 0.1450 - val_loss: 0.8675 - val_acc: 0.1413\n",
            "Epoch 88/200\n",
            " - 0s - loss: 0.1795 - acc: 0.1450 - val_loss: 0.9289 - val_acc: 0.1413\n",
            "Epoch 89/200\n",
            " - 0s - loss: 0.1793 - acc: 0.1450 - val_loss: 0.8851 - val_acc: 0.1413\n",
            "Epoch 90/200\n",
            " - 0s - loss: 0.1790 - acc: 0.1450 - val_loss: 0.8925 - val_acc: 0.1413\n",
            "Epoch 91/200\n",
            " - 0s - loss: 0.1790 - acc: 0.1450 - val_loss: 0.9023 - val_acc: 0.1413\n",
            "Epoch 92/200\n",
            " - 0s - loss: 0.1790 - acc: 0.1450 - val_loss: 0.8809 - val_acc: 0.1413\n",
            "Epoch 93/200\n",
            " - 0s - loss: 0.1790 - acc: 0.1450 - val_loss: 0.8973 - val_acc: 0.1413\n",
            "Epoch 94/200\n",
            " - 0s - loss: 0.1792 - acc: 0.1450 - val_loss: 0.8929 - val_acc: 0.1413\n",
            "Epoch 95/200\n",
            " - 0s - loss: 0.1793 - acc: 0.1450 - val_loss: 0.9194 - val_acc: 0.1413\n",
            "Epoch 96/200\n",
            " - 0s - loss: 0.1790 - acc: 0.1450 - val_loss: 0.9052 - val_acc: 0.1413\n",
            "Epoch 97/200\n",
            " - 0s - loss: 0.1790 - acc: 0.1450 - val_loss: 0.8698 - val_acc: 0.1413\n",
            "Epoch 98/200\n",
            " - 0s - loss: 0.1788 - acc: 0.1450 - val_loss: 0.9278 - val_acc: 0.1413\n",
            "Epoch 99/200\n",
            " - 0s - loss: 0.1788 - acc: 0.1450 - val_loss: 0.8969 - val_acc: 0.1413\n",
            "Epoch 100/200\n",
            " - 0s - loss: 0.1789 - acc: 0.1450 - val_loss: 0.8780 - val_acc: 0.1413\n",
            "Epoch 101/200\n",
            " - 0s - loss: 0.1788 - acc: 0.1450 - val_loss: 0.9297 - val_acc: 0.1413\n",
            "Epoch 102/200\n",
            " - 0s - loss: 0.1791 - acc: 0.1450 - val_loss: 0.9031 - val_acc: 0.1413\n",
            "Epoch 103/200\n",
            " - 0s - loss: 0.1788 - acc: 0.1450 - val_loss: 0.8634 - val_acc: 0.1413\n",
            "Epoch 104/200\n",
            " - 0s - loss: 0.1787 - acc: 0.1450 - val_loss: 0.8943 - val_acc: 0.1413\n",
            "Epoch 105/200\n",
            " - 0s - loss: 0.1788 - acc: 0.1450 - val_loss: 0.8893 - val_acc: 0.1413\n",
            "Epoch 106/200\n",
            " - 0s - loss: 0.1788 - acc: 0.1450 - val_loss: 0.9085 - val_acc: 0.1413\n",
            "Epoch 107/200\n",
            " - 0s - loss: 0.1791 - acc: 0.1450 - val_loss: 0.9356 - val_acc: 0.1413\n",
            "Epoch 108/200\n",
            " - 0s - loss: 0.1790 - acc: 0.1450 - val_loss: 0.8929 - val_acc: 0.1413\n",
            "Epoch 109/200\n",
            " - 0s - loss: 0.1785 - acc: 0.1450 - val_loss: 0.9014 - val_acc: 0.1413\n",
            "Epoch 110/200\n",
            " - 0s - loss: 0.1786 - acc: 0.1450 - val_loss: 0.8843 - val_acc: 0.1413\n",
            "Epoch 111/200\n",
            " - 0s - loss: 0.1786 - acc: 0.1450 - val_loss: 0.8762 - val_acc: 0.1413\n",
            "Epoch 112/200\n",
            " - 0s - loss: 0.1785 - acc: 0.1450 - val_loss: 0.8554 - val_acc: 0.1413\n",
            "Epoch 113/200\n",
            " - 0s - loss: 0.1791 - acc: 0.1450 - val_loss: 0.9241 - val_acc: 0.1413\n",
            "Epoch 114/200\n",
            " - 0s - loss: 0.1785 - acc: 0.1450 - val_loss: 0.8957 - val_acc: 0.1413\n",
            "Epoch 115/200\n",
            " - 0s - loss: 0.1784 - acc: 0.1450 - val_loss: 0.8782 - val_acc: 0.1413\n",
            "Epoch 116/200\n",
            " - 0s - loss: 0.1783 - acc: 0.1450 - val_loss: 0.8635 - val_acc: 0.1413\n",
            "Epoch 117/200\n",
            " - 0s - loss: 0.1784 - acc: 0.1450 - val_loss: 0.8971 - val_acc: 0.1413\n",
            "Epoch 118/200\n",
            " - 0s - loss: 0.1784 - acc: 0.1450 - val_loss: 0.8694 - val_acc: 0.1413\n",
            "Epoch 119/200\n",
            " - 0s - loss: 0.1784 - acc: 0.1450 - val_loss: 0.8934 - val_acc: 0.1413\n",
            "Epoch 120/200\n",
            " - 0s - loss: 0.1784 - acc: 0.1450 - val_loss: 0.8743 - val_acc: 0.1413\n",
            "Epoch 121/200\n",
            " - 0s - loss: 0.1785 - acc: 0.1450 - val_loss: 0.8738 - val_acc: 0.1413\n",
            "Epoch 122/200\n",
            " - 0s - loss: 0.1782 - acc: 0.1450 - val_loss: 0.9039 - val_acc: 0.1413\n",
            "Epoch 123/200\n",
            " - 0s - loss: 0.1784 - acc: 0.1450 - val_loss: 0.8564 - val_acc: 0.1413\n",
            "Epoch 124/200\n",
            " - 0s - loss: 0.1782 - acc: 0.1450 - val_loss: 0.9045 - val_acc: 0.1413\n",
            "Epoch 125/200\n",
            " - 0s - loss: 0.1785 - acc: 0.1450 - val_loss: 0.9256 - val_acc: 0.1413\n",
            "Epoch 126/200\n",
            " - 0s - loss: 0.1788 - acc: 0.1450 - val_loss: 0.8605 - val_acc: 0.1413\n",
            "Epoch 127/200\n",
            " - 0s - loss: 0.1782 - acc: 0.1450 - val_loss: 0.8907 - val_acc: 0.1413\n",
            "Epoch 128/200\n",
            " - 0s - loss: 0.1783 - acc: 0.1450 - val_loss: 0.8958 - val_acc: 0.1413\n",
            "Epoch 129/200\n",
            " - 0s - loss: 0.1782 - acc: 0.1450 - val_loss: 0.8965 - val_acc: 0.1413\n",
            "Epoch 130/200\n",
            " - 0s - loss: 0.1780 - acc: 0.1450 - val_loss: 0.8791 - val_acc: 0.1413\n",
            "Epoch 131/200\n",
            " - 0s - loss: 0.1783 - acc: 0.1450 - val_loss: 0.9174 - val_acc: 0.1413\n",
            "Epoch 132/200\n",
            " - 0s - loss: 0.1783 - acc: 0.1450 - val_loss: 0.8571 - val_acc: 0.1413\n",
            "Epoch 133/200\n",
            " - 0s - loss: 0.1780 - acc: 0.1450 - val_loss: 0.8551 - val_acc: 0.1413\n",
            "Epoch 134/200\n",
            " - 0s - loss: 0.1781 - acc: 0.1450 - val_loss: 0.8349 - val_acc: 0.1413\n",
            "Epoch 135/200\n",
            " - 0s - loss: 0.1783 - acc: 0.1450 - val_loss: 0.8899 - val_acc: 0.1413\n",
            "Epoch 136/200\n",
            " - 0s - loss: 0.1778 - acc: 0.1450 - val_loss: 0.8538 - val_acc: 0.1413\n",
            "Epoch 137/200\n",
            " - 0s - loss: 0.1779 - acc: 0.1450 - val_loss: 0.8901 - val_acc: 0.1413\n",
            "Epoch 138/200\n",
            " - 0s - loss: 0.1780 - acc: 0.1450 - val_loss: 0.8690 - val_acc: 0.1413\n",
            "Epoch 139/200\n",
            " - 0s - loss: 0.1780 - acc: 0.1450 - val_loss: 0.8739 - val_acc: 0.1413\n",
            "Epoch 140/200\n",
            " - 0s - loss: 0.1780 - acc: 0.1450 - val_loss: 0.8678 - val_acc: 0.1413\n",
            "Epoch 141/200\n",
            " - 0s - loss: 0.1779 - acc: 0.1450 - val_loss: 0.8495 - val_acc: 0.1413\n",
            "Epoch 142/200\n",
            " - 0s - loss: 0.1779 - acc: 0.1450 - val_loss: 0.8874 - val_acc: 0.1413\n",
            "Epoch 143/200\n",
            " - 0s - loss: 0.1784 - acc: 0.1450 - val_loss: 0.9166 - val_acc: 0.1413\n",
            "Epoch 144/200\n",
            " - 0s - loss: 0.1779 - acc: 0.1450 - val_loss: 0.8674 - val_acc: 0.1413\n",
            "Epoch 145/200\n",
            " - 0s - loss: 0.1779 - acc: 0.1450 - val_loss: 0.8512 - val_acc: 0.1413\n",
            "Epoch 146/200\n",
            " - 0s - loss: 0.1778 - acc: 0.1450 - val_loss: 0.8603 - val_acc: 0.1413\n",
            "Epoch 147/200\n",
            " - 0s - loss: 0.1778 - acc: 0.1450 - val_loss: 0.8173 - val_acc: 0.1413\n",
            "Epoch 148/200\n",
            " - 0s - loss: 0.1782 - acc: 0.1450 - val_loss: 0.8922 - val_acc: 0.1413\n",
            "Epoch 149/200\n",
            " - 0s - loss: 0.1780 - acc: 0.1450 - val_loss: 0.8724 - val_acc: 0.1413\n",
            "Epoch 150/200\n",
            " - 0s - loss: 0.1779 - acc: 0.1450 - val_loss: 0.8861 - val_acc: 0.1413\n",
            "Epoch 151/200\n",
            " - 0s - loss: 0.1784 - acc: 0.1450 - val_loss: 0.9013 - val_acc: 0.1413\n",
            "Epoch 152/200\n",
            " - 0s - loss: 0.1778 - acc: 0.1450 - val_loss: 0.8446 - val_acc: 0.1413\n",
            "Epoch 153/200\n",
            " - 0s - loss: 0.1776 - acc: 0.1450 - val_loss: 0.8647 - val_acc: 0.1413\n",
            "Epoch 154/200\n",
            " - 0s - loss: 0.1777 - acc: 0.1450 - val_loss: 0.8843 - val_acc: 0.1413\n",
            "Epoch 155/200\n",
            " - 0s - loss: 0.1775 - acc: 0.1450 - val_loss: 0.8872 - val_acc: 0.1413\n",
            "Epoch 156/200\n",
            " - 0s - loss: 0.1776 - acc: 0.1450 - val_loss: 0.8534 - val_acc: 0.1413\n",
            "Epoch 157/200\n",
            " - 0s - loss: 0.1781 - acc: 0.1450 - val_loss: 0.8398 - val_acc: 0.1413\n",
            "Epoch 158/200\n",
            " - 0s - loss: 0.1778 - acc: 0.1450 - val_loss: 0.8784 - val_acc: 0.1413\n",
            "Epoch 159/200\n",
            " - 0s - loss: 0.1778 - acc: 0.1450 - val_loss: 0.8748 - val_acc: 0.1413\n",
            "Epoch 160/200\n",
            " - 0s - loss: 0.1775 - acc: 0.1450 - val_loss: 0.8384 - val_acc: 0.1413\n",
            "Epoch 161/200\n",
            " - 0s - loss: 0.1774 - acc: 0.1450 - val_loss: 0.8662 - val_acc: 0.1413\n",
            "Epoch 162/200\n",
            " - 0s - loss: 0.1776 - acc: 0.1450 - val_loss: 0.8829 - val_acc: 0.1413\n",
            "Epoch 163/200\n",
            " - 0s - loss: 0.1777 - acc: 0.1450 - val_loss: 0.9026 - val_acc: 0.1413\n",
            "Epoch 164/200\n",
            " - 0s - loss: 0.1780 - acc: 0.1450 - val_loss: 0.9045 - val_acc: 0.1413\n",
            "Epoch 165/200\n",
            " - 0s - loss: 0.1777 - acc: 0.1450 - val_loss: 0.8696 - val_acc: 0.1413\n",
            "Epoch 166/200\n",
            " - 0s - loss: 0.1784 - acc: 0.1450 - val_loss: 0.8483 - val_acc: 0.1413\n",
            "Epoch 167/200\n",
            " - 0s - loss: 0.1774 - acc: 0.1450 - val_loss: 0.8685 - val_acc: 0.1413\n",
            "Epoch 168/200\n",
            " - 0s - loss: 0.1775 - acc: 0.1450 - val_loss: 0.8912 - val_acc: 0.1413\n",
            "Epoch 169/200\n",
            " - 0s - loss: 0.1776 - acc: 0.1450 - val_loss: 0.8986 - val_acc: 0.1413\n",
            "Epoch 170/200\n",
            " - 0s - loss: 0.1775 - acc: 0.1450 - val_loss: 0.8743 - val_acc: 0.1413\n",
            "Epoch 171/200\n",
            " - 0s - loss: 0.1774 - acc: 0.1450 - val_loss: 0.8746 - val_acc: 0.1413\n",
            "Epoch 172/200\n",
            " - 0s - loss: 0.1774 - acc: 0.1450 - val_loss: 0.8564 - val_acc: 0.1413\n",
            "Epoch 173/200\n",
            " - 0s - loss: 0.1774 - acc: 0.1450 - val_loss: 0.8715 - val_acc: 0.1413\n",
            "Epoch 174/200\n",
            " - 0s - loss: 0.1773 - acc: 0.1450 - val_loss: 0.8765 - val_acc: 0.1413\n",
            "Epoch 175/200\n",
            " - 0s - loss: 0.1773 - acc: 0.1450 - val_loss: 0.8335 - val_acc: 0.1413\n",
            "Epoch 176/200\n",
            " - 0s - loss: 0.1774 - acc: 0.1450 - val_loss: 0.8431 - val_acc: 0.1413\n",
            "Epoch 177/200\n",
            " - 0s - loss: 0.1774 - acc: 0.1450 - val_loss: 0.8519 - val_acc: 0.1413\n",
            "Epoch 178/200\n",
            " - 0s - loss: 0.1774 - acc: 0.1450 - val_loss: 0.8802 - val_acc: 0.1413\n",
            "Epoch 179/200\n",
            " - 0s - loss: 0.1773 - acc: 0.1450 - val_loss: 0.8555 - val_acc: 0.1413\n",
            "Epoch 180/200\n",
            " - 0s - loss: 0.1775 - acc: 0.1450 - val_loss: 0.8865 - val_acc: 0.1413\n",
            "Epoch 181/200\n",
            " - 0s - loss: 0.1778 - acc: 0.1450 - val_loss: 0.8654 - val_acc: 0.1413\n",
            "Epoch 182/200\n",
            " - 0s - loss: 0.1774 - acc: 0.1450 - val_loss: 0.8627 - val_acc: 0.1413\n",
            "Epoch 183/200\n",
            " - 0s - loss: 0.1774 - acc: 0.1450 - val_loss: 0.8326 - val_acc: 0.1413\n",
            "Epoch 184/200\n",
            " - 0s - loss: 0.1774 - acc: 0.1450 - val_loss: 0.8771 - val_acc: 0.1413\n",
            "Epoch 185/200\n",
            " - 0s - loss: 0.1773 - acc: 0.1450 - val_loss: 0.8242 - val_acc: 0.1413\n",
            "Epoch 186/200\n",
            " - 0s - loss: 0.1772 - acc: 0.1450 - val_loss: 0.8605 - val_acc: 0.1413\n",
            "Epoch 187/200\n",
            " - 0s - loss: 0.1772 - acc: 0.1450 - val_loss: 0.8544 - val_acc: 0.1413\n",
            "Epoch 188/200\n",
            " - 0s - loss: 0.1773 - acc: 0.1450 - val_loss: 0.8671 - val_acc: 0.1413\n",
            "Epoch 189/200\n",
            " - 0s - loss: 0.1774 - acc: 0.1450 - val_loss: 0.8465 - val_acc: 0.1413\n",
            "Epoch 190/200\n",
            " - 0s - loss: 0.1774 - acc: 0.1450 - val_loss: 0.8902 - val_acc: 0.1413\n",
            "Epoch 191/200\n",
            " - 0s - loss: 0.1773 - acc: 0.1450 - val_loss: 0.8624 - val_acc: 0.1413\n",
            "Epoch 192/200\n",
            " - 0s - loss: 0.1774 - acc: 0.1450 - val_loss: 0.8478 - val_acc: 0.1413\n",
            "Epoch 193/200\n",
            " - 0s - loss: 0.1773 - acc: 0.1450 - val_loss: 0.8785 - val_acc: 0.1413\n",
            "Epoch 194/200\n",
            " - 0s - loss: 0.1771 - acc: 0.1450 - val_loss: 0.8505 - val_acc: 0.1413\n",
            "Epoch 195/200\n",
            " - 0s - loss: 0.1771 - acc: 0.1450 - val_loss: 0.8643 - val_acc: 0.1413\n",
            "Epoch 196/200\n",
            " - 0s - loss: 0.1772 - acc: 0.1450 - val_loss: 0.8707 - val_acc: 0.1413\n",
            "Epoch 197/200\n",
            " - 0s - loss: 0.1773 - acc: 0.1450 - val_loss: 0.8757 - val_acc: 0.1413\n",
            "Epoch 198/200\n",
            " - 0s - loss: 0.1771 - acc: 0.1450 - val_loss: 0.8506 - val_acc: 0.1413\n",
            "Epoch 199/200\n",
            " - 0s - loss: 0.1772 - acc: 0.1450 - val_loss: 0.8443 - val_acc: 0.1413\n",
            "Epoch 200/200\n",
            " - 0s - loss: 0.1771 - acc: 0.1450 - val_loss: 0.8774 - val_acc: 0.1413\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUBQ8t8z1j6W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "c0bac47d-48a6-41a0-bfe9-3b58782a57b0"
      },
      "source": [
        "acc_df1 = pd.DataFrame()\n",
        "acc_df1['predict'] = test2.flatten()\n",
        "acc_df1.head()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>predict</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.042751</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.043002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.050843</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.074965</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.104316</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    predict\n",
              "0  0.042751\n",
              "1  0.043002\n",
              "2  0.050843\n",
              "3  0.074965\n",
              "4  0.104316"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LsYSYaJ1ndF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "b4c0b319-673b-4f5e-a49c-cbfea0914c81"
      },
      "source": [
        "acc_df1['labels'] = y2_test\n",
        "acc_df1.head(10)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>predict</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.042751</td>\n",
              "      <td>0.025641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.043002</td>\n",
              "      <td>0.051282</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.050843</td>\n",
              "      <td>0.076923</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.074965</td>\n",
              "      <td>0.105263</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.104316</td>\n",
              "      <td>0.131579</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.044384</td>\n",
              "      <td>0.157895</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.001490</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.051737</td>\n",
              "      <td>0.026316</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.057636</td>\n",
              "      <td>0.052632</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.059519</td>\n",
              "      <td>0.078947</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    predict    labels\n",
              "0  0.042751  0.025641\n",
              "1  0.043002  0.051282\n",
              "2  0.050843  0.076923\n",
              "3  0.074965  0.105263\n",
              "4  0.104316  0.131579\n",
              "5  0.044384  0.157895\n",
              "6  0.001490  0.000000\n",
              "7  0.051737  0.026316\n",
              "8  0.057636  0.052632\n",
              "9  0.059519  0.078947"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMc_m7Gx1qs1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "4786bb63-0552-4dec-ae4d-07bb8160d86d"
      },
      "source": [
        "print(\"Accuracy is: {}\\nDone for this semester!!\".format(1.0*sum((acc_df1['labels']==1)==(acc_df1['predict']>0.5))/len(acc_df1['predict'])))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy is: 0.9814126394052045\n",
            "Done for this semester!!\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}